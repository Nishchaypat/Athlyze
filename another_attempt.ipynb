{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF using Document AI...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "# Set your environment variables for authentication\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"preprocessing_credentials.json\"\n",
    "\n",
    "# Document AI: Extract Text from PDF\n",
    "def extract_text_from_pdf(project_id, location, processor_id, file_path):\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Configure the request\n",
    "    document = documentai.RawDocument(content=content, mime_type=\"application/pdf\")\n",
    "    name = f\"projects/{project_id}/locations/us/processors/{processor_id}\"\n",
    "    request = {\"name\": name, \"raw_document\": document}\n",
    "\n",
    "    # Process the document\n",
    "    response = client.process_document(request=request)\n",
    "    document_text = response.document.text\n",
    "\n",
    "    return document_text\n",
    "\n",
    "# Vertex AI: Semantic Chunking with Gemini\n",
    "def perform_semantic_chunking_with_gemini(project_id, location, text_input):\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "\n",
    "    # Load Gemini model\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "\n",
    "    # Generate embeddings for semantic chunking\n",
    "    embeddings = model.get_embeddings([text_input])\n",
    "\n",
    "    # Dummy implementation for chunking (semantic embeddings clustering)\n",
    "    # For demonstration: Split text into smaller chunks by semantic similarity.\n",
    "    # Replace this with advanced clustering logic as per your use case.\n",
    "    chunk_size = 500  # Characters per chunk\n",
    "    chunks = [text_input[i:i+chunk_size] for i in range(0, len(text_input), chunk_size)]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Main Workflow\n",
    "def main():\n",
    "    # GCP Configuration\n",
    "    project_id = \"athlyze-446917\"\n",
    "    location = \"us-central1\"\n",
    "    processor_id = \"a370be5d003f980f\"\n",
    "    file_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "\n",
    "    # Step 1: Extract text using Document AI\n",
    "    print(\"Extracting text from PDF using Document AI...\")\n",
    "    extracted_text = extract_text_from_pdf(project_id, location, processor_id, file_path)\n",
    "    print(extracted_text)\n",
    "    # Step 2: Perform semantic chunking with Gemini\n",
    "    print(\"Performing semantic chunking with Vertex AI...\")\n",
    "    semantic_chunks = perform_semantic_chunking_with_gemini(project_id, location, extracted_text)\n",
    "\n",
    "    # Step 3: Print the semantic chunks\n",
    "    print(\"\\nSemantic Chunks:\")\n",
    "    for i, chunk in enumerate(semantic_chunks, 1):\n",
    "        print(f\"Chunk {i}:\\n{chunk}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
