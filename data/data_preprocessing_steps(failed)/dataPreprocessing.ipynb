{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import os\n",
    "\n",
    "gemini = os.getenv(\"GEMINI_API_KEY\")\n",
    "location = os.getenv(\"location\")\n",
    "location_processor = os.getenv(\"location_processor\")\n",
    "project_id = os.getenv(\"project_id\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "\n",
    "aiplatform.init(project=project_id, location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agentic Segmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on 1 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# Get requirements\n",
    "\n",
    "gemini = os.getenv(\"GEMINI_API_KEY\")\n",
    "location = os.getenv(\"location\")\n",
    "location_processor = os.getenv(\"location_processor\")\n",
    "project_id = os.getenv(\"project_id\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    model = GoogleGenerativeAI(\n",
    "        model=\"gemini-1.0-pro\",\n",
    "        google_api_key=gemini,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def clean_response(response_str):\n",
    "    \"\"\"\n",
    "    Cleans the response string by removing the code block markers and then attempts to convert it to JSON.\n",
    "    \"\"\"\n",
    "    # Remove the code block markers (start and end)\n",
    "    response_str = re.sub(r'^```json\\n', '', response_str)\n",
    "    response_str = re.sub(r'```$', '', response_str).strip()\n",
    "    \n",
    "    # Attempt to parse the cleaned string into a JSON object\n",
    "    try:\n",
    "        response_json = json.loads(response_str)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing the response as JSON, response was:\", response_str)\n",
    "        return {\"findings\": [], \"metadata\": {}}\n",
    "\n",
    "def chunk_and_clean_text(model, raw_text):\n",
    "    prompt = \"\"\"\n",
    "    You are a highly capable AI model tasked with cleaning and chunking the provided text.\n",
    "    Please return the response in JSON format with two keys:\n",
    "    - \"findings\": A list of valid claims or facts related to muscle training, nutrition, gym, biology, etc, this claims should be unique and shall not be a simple fact, it should be worth getting from a \"secietific\" research paper.\n",
    "    - \"metadata\": A dictionary containing the \"title\" key with the paper's title.\n",
    "    Here is the input text: {raw_text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get response from Gemini model (in string format)\n",
    "    response_str = model(prompt.format(raw_text=raw_text))\n",
    "        \n",
    "    # Clean and parse the response string into a JSON object\n",
    "    response_json = clean_response(response_str)\n",
    "    \n",
    "    findings = response_json.get(\"findings\", [])\n",
    "    metadata = response_json.get(\"metadata\", {})\n",
    "    \n",
    "    return {\"findings\": findings, \"metadata\": metadata}\n",
    "\n",
    "def process_pages(pages):\n",
    "    model = initialize_model()\n",
    "    full_response = {\"findings\": [], \"metadata\": {}}\n",
    "    \n",
    "    for page in pages:\n",
    "        print(f\"Processing page {pages.index(page) + 1}...\")\n",
    "        response = chunk_and_clean_text(model, page)\n",
    "        print(page)\n",
    "        # Print the response for debugging purposes\n",
    "        print(\"Response:\", response)  # Print the response to verify it's in the correct format\n",
    "        \n",
    "        # Merge findings from the response\n",
    "        if isinstance(response, dict):\n",
    "            # Append findings to full_response['findings']\n",
    "            full_response[\"findings\"].extend(response.get(\"findings\", []))\n",
    "            \n",
    "            # Merge metadata if it's not already set\n",
    "            if not full_response[\"metadata\"]:\n",
    "                full_response[\"metadata\"] = response.get(\"metadata\", {})\n",
    "        else:\n",
    "            print(\"Response is not in the expected format:\", response)\n",
    "        break\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Extract text from PDF using PyPDF2\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages = []\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            pages.append(page_text)\n",
    "    return pages\n",
    "\n",
    "\n",
    "# Path to your PDF file\n",
    "file_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "# Extract text from the PDF using PyPDF2\n",
    "print(\"Extracting text from PDF using PyPDF2...\")\n",
    "pages = extract_text_from_pdf(file_path)\n",
    "print(len(pages))\n",
    "# Perform chunking and cleaning\n",
    "print(\"Cleaning and chunking text from each page...\")\n",
    "final_response = process_pages(pages)\n",
    "\n",
    "# Output the final response\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above to all the files in the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import PyPDF2\n",
    "\n",
    "# Initialize the Gemini Pro model\n",
    "def initialize_model():\n",
    "    try:\n",
    "        gemini = os.getenv(\"GEMINI_API_KEY\")\n",
    "        model = GoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            google_api_key=gemini,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        print(\"Model initialized successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def clean_response(response_str):\n",
    "    # Remove code block markers if present\n",
    "    response_str = re.sub(r'^```json\\n', '', response_str)\n",
    "    response_str = re.sub(r'```$', '', response_str).strip()\n",
    "    \n",
    "    # Ensure response ends with proper JSON structure\n",
    "    if not response_str.endswith(\"}\"):\n",
    "        print(\"Warning: Response appears to be incomplete. Attempting to fix...\")\n",
    "        response_str += \"}\"\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing response as JSON: {response_str[:500]}\")  # Log first 500 characters\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        return {\"findings\": []}\n",
    "\n",
    "\n",
    "# Chunk text and send it to Gemini Pro\n",
    "def chunk_and_clean_text(model, raw_text, title=\"Unknown Document\"):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=4000,  # Adjust to Gemini's input limit\n",
    "        chunk_overlap=200  # Overlap between chunks for context preservation\n",
    "    )\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    combined_response = {\"findings\": []}\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {idx + 1}/{len(chunks)}...\")\n",
    "        prompt = f\"\"\"\n",
    "        You are a highly capable AI assisting with extracting relevant information.\n",
    "        - Extract valid claims or facts related to muscle training, nutrition, gym, biology, etc.\n",
    "        - Return the response as a **valid JSON object** having one key:\n",
    "          - \"findings\": List of claims/facts that should be not too simple and hold very relevant info.\n",
    "        Here is the input text: {chunk}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response_str = model(prompt)\n",
    "            response = clean_response(response_str)\n",
    "            combined_response[\"findings\"].extend(response.get(\"findings\", []))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {idx + 1}: {e}\")\n",
    "    \n",
    "    return combined_response\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            return [page.extract_text() for page in pdf_reader.pages]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process all PDFs in the specified folder\n",
    "def process_folder(folder_path):\n",
    "    model = initialize_model()\n",
    "    aggregated_results = {\"findings\": []}\n",
    "    failed_files = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            print(f\"\\nProcessing file: {file_name}\")\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            pages = extract_text_from_pdf(file_path)\n",
    "            \n",
    "            if not pages:\n",
    "                print(f\"Skipping {file_name}: No readable pages found.\")\n",
    "                failed_files.append(file_name)\n",
    "                continue\n",
    "\n",
    "            full_text = \"\\n\".join(pages)\n",
    "            file_response = chunk_and_clean_text(model, full_text, title=file_name)\n",
    "            aggregated_results[\"findings\"].extend(file_response.get(\"findings\", []))\n",
    "            print(f\"Finished processing {file_name}.\")\n",
    "            print(f\"Total findings so far: {len(aggregated_results['findings'])}\")\n",
    "\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Failed to process {len(failed_files)} files: {failed_files}\")\n",
    "    return aggregated_results\n",
    "\n",
    "# Main execution\n",
    "folder_path = \"nutrition_research_papers\"  # Folder containing PDF files\n",
    "print(\"Starting processing for folder:\", folder_path)\n",
    "\n",
    "final_results = process_folder(folder_path)\n",
    "print(final_results)\n",
    "# Format for embedding-friendly output\n",
    "formatted_results = [\n",
    "    {\n",
    "        \"id\": idx + 1,\n",
    "        \"text\": finding,\n",
    "        \"embedding\": None,  # Placeholder; replace after embedding.\n",
    "    }\n",
    "    for idx, finding in enumerate(final_results[\"findings\"])\n",
    "]\n",
    "print(formatted_results)\n",
    "# Save the formatted results to JSON\n",
    "output_file_path = \"formatted_embeddings_input_nutrition.json\"\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(formatted_results, output_file, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to {output_file_path}.\")\n",
    "print(f\"Total findings: {len(formatted_results)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
