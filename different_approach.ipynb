{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    # Initialize the Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Specify the processor name (replace with your actual processor ID)\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    # Create the raw document request\n",
    "    raw_document = documentai.RawDocument(content=pdf_content)\n",
    "\n",
    "    # Create the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # Extract and return the text\n",
    "    document = result.document\n",
    "    text = document.text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "    response = client.classify_text(document=document)\n",
    "\n",
    "    filtered_text = []\n",
    "    for category in response.categories:\n",
    "        if any(cat in category.name for cat in categories_to_keep):\n",
    "            filtered_text.append(text)\n",
    "\n",
    "    return \" \".join(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def agentic_chunking(text, embedding_model):\n",
    "    # Split text into smaller parts for processing\n",
    "    doc = Document(page_content=text)\n",
    "\n",
    "    # Use a summarization chain to group text into chunks\n",
    "    summarize_chain = load_summarize_chain(embedding_model)\n",
    "    output = summarize_chain.run([doc])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbedding\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embedding_model = TextEmbedding.from_pretrained(\"textembedding-gecko\")\n",
    "    return embedding_model.embed(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file):\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(file_path, project_id)\n",
    "\n",
    "    # Filter useful content\n",
    "    filtered_text = filter_relevant_content(text, categories_to_keep)\n",
    "\n",
    "    # Perform agentic chunking\n",
    "    chunks = agentic_chunking(filtered_text, embedding_model=\"gemini-text-embedding\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    chunk_data = [{\"chunk\": chunk, \"embedding\": generate_embeddings(chunk)} for chunk in chunks]\n",
    "\n",
    "    # Save chunks to file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_research_paper(\n",
    "    \"nutrition_research_papers/nutrients-11-01136.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from langchain.chains import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from vertexai.language_models import TextEmbedding\n",
    "\n",
    "# Set the environment variable for GCP credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'preprocessing_credentials.json'\n",
    "\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    # Initialize the Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Specify the processor name (replace with your actual processor ID)\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    # Create the raw document request\n",
    "    raw_document = documentai.RawDocument(content=pdf_content)\n",
    "\n",
    "    # Create the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # Extract and return the text\n",
    "    document = result.document\n",
    "    text = document.text\n",
    "    return text\n",
    "\n",
    "def split_document_into_chunks(file_path, max_pages=15):\n",
    "    # This function splits the PDF into smaller chunks based on max_pages\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    # Read the PDF document\n",
    "    reader = PdfReader(file_path)\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "    chunks = []\n",
    "    for start_page in range(0, num_pages, max_pages):\n",
    "        end_page = min(start_page + max_pages, num_pages)\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # Combine text from the pages in the current chunk\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page = reader.pages[page_num]\n",
    "            chunk += page.extract_text()\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def agentic_chunking(text, embedding_model):\n",
    "    # Split text into smaller parts for processing\n",
    "    doc = Document(page_content=text)\n",
    "\n",
    "    # Use a summarization chain to group text into chunks\n",
    "    summarize_chain = load_summarize_chain(embedding_model)\n",
    "    output = summarize_chain.run([doc])\n",
    "\n",
    "    return output\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    # Generate embeddings using the Gemini model\n",
    "    embedding_model = TextEmbedding.from_pretrained(\"textembedding-gecko\")\n",
    "    return embedding_model.embed(text)\n",
    "\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    # Save the chunks and embeddings to a JSON file\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n",
    "\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    # Filter the text based on the categories provided\n",
    "    # You can implement a more specific filtering method here if needed\n",
    "    filtered_text = \"\\n\".join([line for line in text.splitlines() if any(category in line for category in categories_to_keep)])\n",
    "    return filtered_text\n",
    "\n",
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file, max_pages=15):\n",
    "    # Split the document into smaller chunks if it's too large\n",
    "    chunks = split_document_into_chunks(file_path, max_pages)\n",
    "    \n",
    "    # Combine extracted text from all chunks\n",
    "    combined_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        # Filter useful content based on categories\n",
    "        filtered_text = filter_relevant_content(chunk, categories_to_keep)\n",
    "\n",
    "        # Combine the filtered text\n",
    "        combined_text += filtered_text\n",
    "\n",
    "    # Perform agentic chunking\n",
    "    chunked_text = agentic_chunking(combined_text, embedding_model=\"gemini-text-embedding\")\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    chunk_data = [{\"chunk\": chunk, \"embedding\": generate_embeddings(chunk)} for chunk in chunked_text]\n",
    "\n",
    "    # Save chunks and embeddings to a file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_research_paper(\n",
    "    \"nutrition_research_papers/nutrients-11-01136.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "# Set up the API credentials (ensure your Google Cloud credentials are set)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "aiplatform.init(project=\"athlyze-446917\", location=\"us-central1\")  # Replace with your project ID and region\n",
    "\n",
    "# Function to extract text from PDF using Document AI\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    raw_document = documentai.RawDocument(\n",
    "        content=pdf_content,\n",
    "        mime_type=\"application/pdf\"\n",
    "    )\n",
    "\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "    document = result.document\n",
    "    return document.text\n",
    "\n",
    "# Function to filter relevant content\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    \"\"\"Filter the content based on categories (customize as needed).\"\"\"\n",
    "    return text if any(cat in text for cat in categories_to_keep) else \"\"\n",
    "\n",
    "# Function to generate embeddings using Vertex AI\n",
    "def generate_embeddings(text):\n",
    "    \"\"\"Generate embeddings using Vertex AI.\"\"\"\n",
    "    # Replace with your actual model and endpoint ID for embeddings\n",
    "    endpoint = aiplatform.Endpoint(\"projects/athlyze-446917/locations/us-central1/endpoints/embedding-endpoint-id\")\n",
    "    response = endpoint.predict(instances=[{\"content\": text}])\n",
    "    return response.predictions\n",
    "\n",
    "# Function to summarize text using Vertex AI\n",
    "def summarize_text(text):\n",
    "    \"\"\"Summarize the text using Vertex AI.\"\"\"\n",
    "    # Replace with your actual model and endpoint ID for summarization\n",
    "    endpoint = aiplatform.Endpoint(\"projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id\")\n",
    "    response = endpoint.predict(instances=[{\"content\": text}])\n",
    "    return response.predictions[0]['summary']\n",
    "\n",
    "# Function to save the chunks with embeddings to a file\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    \"\"\"Save chunked data with embeddings to a JSON file.\"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n",
    "\n",
    "# Main function to process research paper and generate vector database\n",
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file):\n",
    "    \"\"\"Process a research paper, clean, chunk, summarize, and generate embeddings.\"\"\"\n",
    "    # Extract text from the paper\n",
    "    text = extract_text_from_pdf(file_path, project_id)\n",
    "\n",
    "    # Filter and clean text based on categories\n",
    "    filtered_text = filter_relevant_content(text, categories_to_keep)\n",
    "\n",
    "    # Summarize the filtered text\n",
    "    summarized_text = summarize_text(filtered_text)\n",
    "\n",
    "    # Generate embeddings for the summarized text\n",
    "    embeddings = generate_embeddings(summarized_text)\n",
    "\n",
    "    # Create chunks with embeddings\n",
    "    chunk_data = [{\"chunk\": summarized_text, \"embedding\": embeddings}]\n",
    "\n",
    "    # Save chunks with embeddings to file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_research_paper(\n",
    "    \"resistant_research_papers/2102.00836v2.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict, Union, Optional\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AthlyzeChunker:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the AthlyzeChunker with configuration.\"\"\"\n",
    "        self.chunks: Dict[str, Dict] = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.categories = [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"]\n",
    "        self.print_logging = True\n",
    "\n",
    "        # Initialize the Google Gemini LLM\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=\"gemini-1.0-pro\",\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_file_path: str) -> str:\n",
    "        \"\"\"Extract raw text from the PDF.\"\"\"\n",
    "        try:\n",
    "            with open(pdf_file_path, 'rb') as file:\n",
    "                reader = PdfReader(file)\n",
    "                return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_and_categorize(self, text: str) -> None:\n",
    "        \"\"\"Extract, clean, and categorize text into meaningful sections.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an assistant categorizing research into sections.\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                f\"Process the following text, clean it (e.g., remove references like [1], figures, and irrelevant details), and categorize it into: {', '.join(self.categories)}. Provide key points for each category.\"\n",
    "            ),\n",
    "            (\"user\", f\"Text: {text.replace('{', '{{').replace('}', '}}')}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format_prompt().to_messages())\n",
    "            cleaned_text = response.get('content', '').strip()\n",
    "            if self.print_logging:\n",
    "                print(\"[INFO] Text processed successfully.\")\n",
    "            self.add_chunks_by_category(cleaned_text)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Text processing failed: {e}\")\n",
    "\n",
    "    def add_chunks_by_category(self, processed_text: str) -> None:\n",
    "        \"\"\"Organize processed text into categories and subcategories.\"\"\"\n",
    "        for category in self.categories:\n",
    "            category_text = self.extract_category_text(processed_text, category)\n",
    "            if category_text:\n",
    "                chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "                self.chunks[chunk_id] = {\n",
    "                    \"category\": category,\n",
    "                    \"content\": category_text,\n",
    "                    \"metadata\": self._generate_metadata(category_text),\n",
    "                    \"chunk_index\": len(self.chunks)\n",
    "                }\n",
    "                if self.print_logging:\n",
    "                    print(f\"[INFO] Added chunk for category '{category}': {chunk_id}\")\n",
    "\n",
    "    def extract_category_text(self, text: str, category: str) -> str:\n",
    "        \"\"\"Extract text specific to a category using Gemini.\"\"\"\n",
    "        try:\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", f\"Extract key details relevant to {category}.\"),\n",
    "                (\"user\", f\"Text: {text.replace('{', '{{').replace('}', '}}')}\")\n",
    "            ])\n",
    "            response = self.llm.invoke(prompt.format_prompt().to_messages())\n",
    "            return response.get('content', '').strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract category text for {category}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _generate_metadata(self, content: str) -> Dict:\n",
    "        \"\"\"Generate metadata for a chunk.\"\"\"\n",
    "        return {\n",
    "            \"content_type\": \"scientific_finding\",\n",
    "            \"creation_time\": str(uuid.uuid1()),\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "\n",
    "    def save_chunks_to_file(self, file_path: str) -> None:\n",
    "        \"\"\"Save categorized chunks to a file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for chunk_id, chunk in self.chunks.items():\n",
    "                    file.write(f\"Chunk ID: {chunk_id}\\n\")\n",
    "                    file.write(f\"Category: {chunk['category']}\\n\")\n",
    "                    file.write(f\"Content:\\n{chunk['content']}\\n\")\n",
    "                    file.write(f\"Metadata: {chunk['metadata']}\\n\")\n",
    "                    file.write(\"\\n---\\n\\n\")\n",
    "            if self.print_logging:\n",
    "                print(f\"[INFO] Chunks saved to file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to save chunks: {e}\")\n",
    "\n",
    "# Main Function for Athlyze\n",
    "def process_research_paper_for_athlyze(pdf_path: str, output_file: str) -> None:\n",
    "    \"\"\"Extract and categorize research data into sections for Athlyze.\"\"\"\n",
    "    try:\n",
    "        chunker = AthlyzeChunker()\n",
    "        raw_text = chunker.extract_text_from_pdf(pdf_path)\n",
    "        chunker.process_and_categorize(raw_text)\n",
    "        chunker.save_chunks_to_file(output_file)\n",
    "        print(f\"[INFO] Processing completed. Results saved to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process research paper: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Replace with your PDF path\n",
    "output_file = \"athlyze_chunks.txt\"\n",
    "process_research_paper_for_athlyze(pdf_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Project `543798683069` is not allowed to use Publisher Model `projects/athlyze-446917/locations/us-central1/publishers/google/models/text-bison@001`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"Project `543798683069` is not allowed to use Publisher Model `projects/athlyze-446917/locations/us-central1/publishers/google/models/text-bison@001`\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.138.95:443 {created_time:\"2025-01-19T11:48:15.14451-05:00\", grpc_status:9, grpc_message:\"Project `543798683069` is not allowed to use Publisher Model `projects/athlyze-446917/locations/us-central1/publishers/google/models/text-bison@001`\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m     inspector\u001b[38;5;241m.\u001b[39minspect_chunks(pdf_path)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 304\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m inspector \u001b[38;5;241m=\u001b[39m ChunkInspector(processor)\n\u001b[1;32m    303\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresistant_research_papers/2102.00836v2.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 304\u001b[0m \u001b[43minspector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 254\u001b[0m, in \u001b[0;36mChunkInspector.inspect_chunks\u001b[0;34m(self, pdf_path, max_chunks)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minspect_chunks\u001b[39m(\u001b[38;5;28mself\u001b[39m, pdf_path: \u001b[38;5;28mstr\u001b[39m, max_chunks: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Inspect chunks and embeddings before upload.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     processed_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mvectorize_chunks(processed_chunks)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsole\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[bold blue]Chunk Processing Summary:[/bold blue]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 202\u001b[0m, in \u001b[0;36mAcademicPaperProcessor.process_pdf\u001b[0;34m(self, pdf_path)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mpages:  \u001b[38;5;66;03m# Iterate over the pages\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_text()  \u001b[38;5;66;03m# Use extract_text()\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magentic_chunk_text(text)\n\u001b[1;32m    205\u001b[0m processed_chunks \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[25], line 104\u001b[0m, in \u001b[0;36mAcademicPaperProcessor.extract_metadata\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract metadata from paper header/title section.\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124mFocus on accuracy and completeness.\u001b[39m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 104\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28meval\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:1417\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences, candidate_count, grounding_source, logprobs, presence_penalty, frequency_penalty, logit_bias, seed)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \n\u001b[1;32m   1362\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;124;03m    A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m prediction_request \u001b[38;5;241m=\u001b[39m _create_text_generation_prediction_request(\n\u001b[1;32m   1402\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1403\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39mmax_output_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1415\u001b[0m )\n\u001b[0;32m-> 1417\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprediction_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse_text_generation_model_multi_candidate_response(\n\u001b[1;32m   1423\u001b[0m     prediction_response\n\u001b[1;32m   1424\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:2293\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   2285\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2286\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         model_version_id\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2290\u001b[0m     )\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2293\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m   2300\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:887\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Project `543798683069` is not allowed to use Publisher Model `projects/athlyze-446917/locations/us-central1/publishers/google/models/text-bison@001`"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel \n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    text: str\n",
    "    use_case: str\n",
    "    metadata: Dict\n",
    "    section_type: str\n",
    "    relevance_score: float\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    methodology_details: Optional[Dict]\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two text chunks.\"\"\"\n",
    "        tfidf_matrix = self.tfidf.fit_transform([text1, text2])\n",
    "        return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.3) -> List[str]:\n",
    "        \"\"\"Merge chunks that are semantically similar.\"\"\"\n",
    "        merged_chunks = []\n",
    "        current_chunk = chunks[0]\n",
    "\n",
    "        for next_chunk in chunks[1:]:\n",
    "            similarity = self.get_semantic_similarity(current_chunk, next_chunk)\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk = f\"{current_chunk}\\n{next_chunk}\"\n",
    "            else:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks\n",
    "\n",
    "class AcademicPaperProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str = \"us-central1\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        self.project_id = project_id\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "\n",
    "    def extract_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract metadata from paper header/title section.\"\"\"\n",
    "        prompt = \"\"\"You are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\n",
    "        Focus on accuracy and completeness.\n",
    "\n",
    "        Required fields:\n",
    "        1. Title (exact paper title)\n",
    "        2. Authors (full list with affiliations if available)\n",
    "        3. Publication details:\n",
    "           - Year\n",
    "           - Journal/Conference\n",
    "           - DOI\n",
    "           - Volume/Issue\n",
    "        4. Keywords (if present)\n",
    "        5. Research domain/field\n",
    "\n",
    "        Format the response as a valid Python dictionary.\n",
    "\n",
    "        Text to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(text=text[:3000]),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            return eval(response.text)\n",
    "        except:\n",
    "            return {\"title\": \"Unknown\", \"authors\": [], \"year\": None}\n",
    "\n",
    "    def analyze_chunk(self, chunk: str, metadata: Dict) -> ProcessedChunk:\n",
    "        \"\"\"Analyze chunk content with enhanced agentic understanding.\"\"\"\n",
    "        prompt = \"\"\"You are an expert research analyst. Analyze this academic paper excerpt.\n",
    "\n",
    "        Task: Extract and structure the following components:\n",
    "\n",
    "        1. Core Scientific Content:\n",
    "           - Main findings or theoretical concepts\n",
    "           - Methodologies or approaches\n",
    "           - Evidence supporting claims\n",
    "\n",
    "        2. Practical Applications:\n",
    "           - How this information can be applied\n",
    "\n",
    "        3. Critical Analysis:\n",
    "           - Scientific validity (0-1)\n",
    "           - Practical applicability (0-1)\n",
    "           - Identify limitations or constraints\n",
    "\n",
    "        Format response as a Python dictionary with keys:\n",
    "        {\n",
    "            'text': 'core scientific content',\n",
    "            'use_case': 'practical applications',\n",
    "            'section_type': 'type of section',\n",
    "            'relevance_score': float,\n",
    "            'key_findings': [list of findings],\n",
    "            'citations': [list of referenced papers],\n",
    "            'methodology_details': {dict of methods}\n",
    "        }\n",
    "\n",
    "        Text to analyze:\n",
    "        {chunk}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(chunk=chunk),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = eval(response.text)\n",
    "            return ProcessedChunk(\n",
    "                text=result['text'],\n",
    "                use_case=result['use_case'],\n",
    "                metadata=metadata,\n",
    "                section_type=result['section_type'],\n",
    "                relevance_score=result['relevance_score'],\n",
    "                key_findings=result['key_findings'],\n",
    "                citations=result['citations'],\n",
    "                methodology_details=result['methodology_details']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            return ProcessedChunk(\n",
    "                text=chunk,\n",
    "                use_case=\"\",\n",
    "                metadata=metadata,\n",
    "                section_type=\"unknown\",\n",
    "                relevance_score=0.0,\n",
    "                key_findings=[],\n",
    "                citations=[],\n",
    "                methodology_details=None\n",
    "            )\n",
    "\n",
    "    def agentic_chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Perform intelligent chunking based on semantic meaning.\"\"\"\n",
    "        section_pattern = r'\\n#{1,3}\\s+[A-Z].*?\\n'\n",
    "        sections = re.split(section_pattern, text)\n",
    "\n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            initial_chunks = self.text_splitter.create_documents([section])\n",
    "            chunk_texts = [chunk.page_content for chunk in initial_chunks]\n",
    "            merged_chunks = self.semantic_chunker.merge_similar_chunks(chunk_texts)\n",
    "            chunks.extend(merged_chunks)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[ProcessedChunk]:\n",
    "        \"\"\"Process a PDF file with enhanced chunking.\"\"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            doc = PyPDF2.PdfReader(file)  # Pass the file object, not just the path\n",
    "            text = \"\"\n",
    "            for page in doc.pages:  # Iterate over the pages\n",
    "                text += page.extract_text()  # Use extract_text()\n",
    "\n",
    "        metadata = self.extract_metadata(text)\n",
    "        chunks = self.agentic_chunk_text(text)\n",
    "\n",
    "        processed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            processed_chunk = self.analyze_chunk(chunk, metadata)\n",
    "            if processed_chunk.relevance_score > 0.6:\n",
    "                processed_chunks.append(processed_chunk)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "    def vectorize_chunks(self, chunks: List[ProcessedChunk]) -> List[Dict]:\n",
    "        \"\"\"Enhanced vectorization with semantic understanding.\"\"\"\n",
    "        vectorized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            combined_text = f\"\"\"\n",
    "            Content: {chunk.text}\n",
    "\n",
    "            Key Findings: {' | '.join(chunk.key_findings)}\n",
    "\n",
    "            Practical Applications: {chunk.use_case}\n",
    "\n",
    "            Methodology: {chunk.methodology_details if chunk.methodology_details else 'Not specified'}\n",
    "\n",
    "            Section Type: {chunk.section_type}\n",
    "            \"\"\"\n",
    "\n",
    "            metadata = {\n",
    "                **chunk.metadata,\n",
    "                \"section_type\": chunk.section_type,\n",
    "                \"relevance_score\": chunk.relevance_score,\n",
    "                \"use_case\": chunk.use_case,\n",
    "                \"key_findings\": chunk.key_findings,\n",
    "                \"citations\": chunk.citations,\n",
    "                \"methodology_details\": chunk.methodology_details\n",
    "            }\n",
    "\n",
    "            vectorized_chunks.append({\n",
    "                \"text\": combined_text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "        return vectorized_chunks\n",
    "\n",
    "class ChunkInspector:\n",
    "    def __init__(self, processor: AcademicPaperProcessor):\n",
    "        self.processor = processor\n",
    "        self.console = Console()\n",
    "\n",
    "    def inspect_chunks(self, pdf_path: str, max_chunks: int = 5):\n",
    "        \"\"\"Inspect chunks and embeddings before upload.\"\"\"\n",
    "        processed_chunks = self.processor.process_pdf(pdf_path)\n",
    "        vectors = self.processor.vectorize_chunks(processed_chunks)\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Chunk Processing Summary:[/bold blue]\")\n",
    "        self.console.print(f\"Total chunks extracted: {len(vectors)}\")\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Sample Chunks:[/bold blue]\")\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Chunk #\", style=\"dim\")\n",
    "        table.add_column(\"Text Preview\")\n",
    "        table.add_column(\"Relevance Score\")\n",
    "        table.add_column(\"Section Type\")\n",
    "        table.add_column(\"Key Findings Count\")\n",
    "\n",
    "        for i, vector in enumerate(vectors[:max_chunks]):\n",
    "            metadata = vector['metadata']\n",
    "            table.add_row(\n",
    "                str(i + 1),\n",
    "                vector['text'][:100] + \"...\",\n",
    "                f\"{metadata['relevance_score']:.2f}\",\n",
    "                metadata['section_type'],\n",
    "                str(len(metadata.get('key_findings', [])))\n",
    "            )\n",
    "\n",
    "        self.console.print(table)\n",
    "\n",
    "    def export_to_csv(self, vectors: List[Dict], output_path: str):\n",
    "        \"\"\"Export chunks and metadata to CSV.\"\"\"\n",
    "        records = []\n",
    "        for vector in vectors:\n",
    "            metadata = vector['metadata']\n",
    "            records.append({\n",
    "                'text_preview': vector['text'][:200],\n",
    "                'section_type': metadata['section_type'],\n",
    "                'relevance_score': metadata['relevance_score'],\n",
    "                'key_findings_count': len(metadata.get('key_findings', []))\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        self.console.print(f\"\\n[green]Exported analysis to {output_path}[/green]\")\n",
    "\n",
    "def main():\n",
    "    processor = AcademicPaperProcessor(\n",
    "        project_id=\"athlyze-446917\",\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    inspector = ChunkInspector(processor)\n",
    "\n",
    "    pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "    inspector.inspect_chunks(pdf_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "print(aiplatform.init(project=\"athlyze-446917\", location=\"us-central1\"))\n",
    "models = aiplatform.Model.list()\n",
    "for model in models:\n",
    "    print(model.resource_name, model.display_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
