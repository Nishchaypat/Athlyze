{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    # Initialize the Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Specify the processor name (replace with your actual processor ID)\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    # Create the raw document request\n",
    "    raw_document = documentai.RawDocument(content=pdf_content)\n",
    "\n",
    "    # Create the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # Extract and return the text\n",
    "    document = result.document\n",
    "    text = document.text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "    response = client.classify_text(document=document)\n",
    "\n",
    "    filtered_text = []\n",
    "    for category in response.categories:\n",
    "        if any(cat in category.name for cat in categories_to_keep):\n",
    "            filtered_text.append(text)\n",
    "\n",
    "    return \" \".join(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def agentic_chunking(text, embedding_model):\n",
    "    # Split text into smaller parts for processing\n",
    "    doc = Document(page_content=text)\n",
    "\n",
    "    # Use a summarization chain to group text into chunks\n",
    "    summarize_chain = load_summarize_chain(embedding_model)\n",
    "    output = summarize_chain.run([doc])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbedding\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embedding_model = TextEmbedding.from_pretrained(\"textembedding-gecko\")\n",
    "    return embedding_model.embed(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file):\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(file_path, project_id)\n",
    "\n",
    "    # Filter useful content\n",
    "    filtered_text = filter_relevant_content(text, categories_to_keep)\n",
    "\n",
    "    # Perform agentic chunking\n",
    "    chunks = agentic_chunking(filtered_text, embedding_model=\"gemini-text-embedding\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    chunk_data = [{\"chunk\": chunk, \"embedding\": generate_embeddings(chunk)} for chunk in chunks]\n",
    "\n",
    "    # Save chunks to file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_research_paper(\n",
    "    \"nutrition_research_papers/nutrients-11-01136.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from langchain.chains import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from vertexai.language_models import TextEmbedding\n",
    "\n",
    "# Set the environment variable for GCP credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'preprocessing_credentials.json'\n",
    "\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    # Initialize the Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Specify the processor name (replace with your actual processor ID)\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    # Create the raw document request\n",
    "    raw_document = documentai.RawDocument(content=pdf_content)\n",
    "\n",
    "    # Create the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # Extract and return the text\n",
    "    document = result.document\n",
    "    text = document.text\n",
    "    return text\n",
    "\n",
    "def split_document_into_chunks(file_path, max_pages=15):\n",
    "    # This function splits the PDF into smaller chunks based on max_pages\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    # Read the PDF document\n",
    "    reader = PdfReader(file_path)\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "    chunks = []\n",
    "    for start_page in range(0, num_pages, max_pages):\n",
    "        end_page = min(start_page + max_pages, num_pages)\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # Combine text from the pages in the current chunk\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page = reader.pages[page_num]\n",
    "            chunk += page.extract_text()\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def agentic_chunking(text, embedding_model):\n",
    "    # Split text into smaller parts for processing\n",
    "    doc = Document(page_content=text)\n",
    "\n",
    "    # Use a summarization chain to group text into chunks\n",
    "    summarize_chain = load_summarize_chain(embedding_model)\n",
    "    output = summarize_chain.run([doc])\n",
    "\n",
    "    return output\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    # Generate embeddings using the Gemini model\n",
    "    embedding_model = TextEmbedding.from_pretrained(\"textembedding-gecko\")\n",
    "    return embedding_model.embed(text)\n",
    "\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    # Save the chunks and embeddings to a JSON file\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n",
    "\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    # Filter the text based on the categories provided\n",
    "    # You can implement a more specific filtering method here if needed\n",
    "    filtered_text = \"\\n\".join([line for line in text.splitlines() if any(category in line for category in categories_to_keep)])\n",
    "    return filtered_text\n",
    "\n",
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file, max_pages=15):\n",
    "    # Split the document into smaller chunks if it's too large\n",
    "    chunks = split_document_into_chunks(file_path, max_pages)\n",
    "    \n",
    "    # Combine extracted text from all chunks\n",
    "    combined_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        # Filter useful content based on categories\n",
    "        filtered_text = filter_relevant_content(chunk, categories_to_keep)\n",
    "\n",
    "        # Combine the filtered text\n",
    "        combined_text += filtered_text\n",
    "\n",
    "    # Perform agentic chunking\n",
    "    chunked_text = agentic_chunking(combined_text, embedding_model=\"gemini-text-embedding\")\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    chunk_data = [{\"chunk\": chunk, \"embedding\": generate_embeddings(chunk)} for chunk in chunked_text]\n",
    "\n",
    "    # Save chunks and embeddings to a file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_research_paper(\n",
    "    \"nutrition_research_papers/nutrients-11-01136.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "# Set up the API credentials (ensure your Google Cloud credentials are set)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "aiplatform.init(project=\"athlyze-446917\", location=\"us-central1\")  # Replace with your project ID and region\n",
    "\n",
    "# Function to extract text from PDF using Document AI\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    raw_document = documentai.RawDocument(\n",
    "        content=pdf_content,\n",
    "        mime_type=\"application/pdf\"\n",
    "    )\n",
    "\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "    document = result.document\n",
    "    return document.text\n",
    "\n",
    "# Function to filter relevant content\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    \"\"\"Filter the content based on categories (customize as needed).\"\"\"\n",
    "    return text if any(cat in text for cat in categories_to_keep) else \"\"\n",
    "\n",
    "# Function to generate embeddings using Vertex AI\n",
    "def generate_embeddings(text):\n",
    "    \"\"\"Generate embeddings using Vertex AI.\"\"\"\n",
    "    # Replace with your actual model and endpoint ID for embeddings\n",
    "    endpoint = aiplatform.Endpoint(\"projects/athlyze-446917/locations/us-central1/endpoints/embedding-endpoint-id\")\n",
    "    response = endpoint.predict(instances=[{\"content\": text}])\n",
    "    return response.predictions\n",
    "\n",
    "# Function to summarize text using Vertex AI\n",
    "def summarize_text(text):\n",
    "    \"\"\"Summarize the text using Vertex AI.\"\"\"\n",
    "    # Replace with your actual model and endpoint ID for summarization\n",
    "    endpoint = aiplatform.Endpoint(\"projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id\")\n",
    "    response = endpoint.predict(instances=[{\"content\": text}])\n",
    "    return response.predictions[0]['summary']\n",
    "\n",
    "# Function to save the chunks with embeddings to a file\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    \"\"\"Save chunked data with embeddings to a JSON file.\"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n",
    "\n",
    "# Main function to process research paper and generate vector database\n",
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file):\n",
    "    \"\"\"Process a research paper, clean, chunk, summarize, and generate embeddings.\"\"\"\n",
    "    # Extract text from the paper\n",
    "    text = extract_text_from_pdf(file_path, project_id)\n",
    "\n",
    "    # Filter and clean text based on categories\n",
    "    filtered_text = filter_relevant_content(text, categories_to_keep)\n",
    "\n",
    "    # Summarize the filtered text\n",
    "    summarized_text = summarize_text(filtered_text)\n",
    "\n",
    "    # Generate embeddings for the summarized text\n",
    "    embeddings = generate_embeddings(summarized_text)\n",
    "\n",
    "    # Create chunks with embeddings\n",
    "    chunk_data = [{\"chunk\": summarized_text, \"embedding\": embeddings}]\n",
    "\n",
    "    # Save chunks with embeddings to file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_research_paper(\n",
    "    \"resistant_research_papers/2102.00836v2.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict, Union, Optional\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AthlyzeChunker:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the AthlyzeChunker with configuration.\"\"\"\n",
    "        self.chunks: Dict[str, Dict] = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.categories = [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"]\n",
    "        self.print_logging = True\n",
    "\n",
    "        # Initialize the Google Gemini LLM\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=\"gemini-1.0-pro\",\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_file_path: str) -> str:\n",
    "        \"\"\"Extract raw text from the PDF.\"\"\"\n",
    "        try:\n",
    "            with open(pdf_file_path, 'rb') as file:\n",
    "                reader = PdfReader(file)\n",
    "                return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_and_categorize(self, text: str) -> None:\n",
    "        \"\"\"Extract, clean, and categorize text into meaningful sections.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an assistant categorizing research into sections.\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                f\"Process the following text, clean it (e.g., remove references like [1], figures, and irrelevant details), and categorize it into: {', '.join(self.categories)}. Provide key points for each category.\"\n",
    "            ),\n",
    "            (\"user\", f\"Text: {text.replace('{', '{{').replace('}', '}}')}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format_prompt().to_messages())\n",
    "            cleaned_text = response.get('content', '').strip()\n",
    "            if self.print_logging:\n",
    "                print(\"[INFO] Text processed successfully.\")\n",
    "            self.add_chunks_by_category(cleaned_text)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Text processing failed: {e}\")\n",
    "\n",
    "    def add_chunks_by_category(self, processed_text: str) -> None:\n",
    "        \"\"\"Organize processed text into categories and subcategories.\"\"\"\n",
    "        for category in self.categories:\n",
    "            category_text = self.extract_category_text(processed_text, category)\n",
    "            if category_text:\n",
    "                chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "                self.chunks[chunk_id] = {\n",
    "                    \"category\": category,\n",
    "                    \"content\": category_text,\n",
    "                    \"metadata\": self._generate_metadata(category_text),\n",
    "                    \"chunk_index\": len(self.chunks)\n",
    "                }\n",
    "                if self.print_logging:\n",
    "                    print(f\"[INFO] Added chunk for category '{category}': {chunk_id}\")\n",
    "\n",
    "    def extract_category_text(self, text: str, category: str) -> str:\n",
    "        \"\"\"Extract text specific to a category using Gemini.\"\"\"\n",
    "        try:\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", f\"Extract key details relevant to {category}.\"),\n",
    "                (\"user\", f\"Text: {text.replace('{', '{{').replace('}', '}}')}\")\n",
    "            ])\n",
    "            response = self.llm.invoke(prompt.format_prompt().to_messages())\n",
    "            return response.get('content', '').strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract category text for {category}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _generate_metadata(self, content: str) -> Dict:\n",
    "        \"\"\"Generate metadata for a chunk.\"\"\"\n",
    "        return {\n",
    "            \"content_type\": \"scientific_finding\",\n",
    "            \"creation_time\": str(uuid.uuid1()),\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "\n",
    "    def save_chunks_to_file(self, file_path: str) -> None:\n",
    "        \"\"\"Save categorized chunks to a file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for chunk_id, chunk in self.chunks.items():\n",
    "                    file.write(f\"Chunk ID: {chunk_id}\\n\")\n",
    "                    file.write(f\"Category: {chunk['category']}\\n\")\n",
    "                    file.write(f\"Content:\\n{chunk['content']}\\n\")\n",
    "                    file.write(f\"Metadata: {chunk['metadata']}\\n\")\n",
    "                    file.write(\"\\n---\\n\\n\")\n",
    "            if self.print_logging:\n",
    "                print(f\"[INFO] Chunks saved to file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to save chunks: {e}\")\n",
    "\n",
    "# Main Function for Athlyze\n",
    "def process_research_paper_for_athlyze(pdf_path: str, output_file: str) -> None:\n",
    "    \"\"\"Extract and categorize research data into sections for Athlyze.\"\"\"\n",
    "    try:\n",
    "        chunker = AthlyzeChunker()\n",
    "        raw_text = chunker.extract_text_from_pdf(pdf_path)\n",
    "        chunker.process_and_categorize(raw_text)\n",
    "        chunker.save_chunks_to_file(output_file)\n",
    "        print(f\"[INFO] Processing completed. Results saved to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process research paper: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Replace with your PDF path\n",
    "output_file = \"athlyze_chunks.txt\"\n",
    "process_research_paper_for_athlyze(pdf_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel \n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    text: str\n",
    "    use_case: str\n",
    "    metadata: Dict\n",
    "    section_type: str\n",
    "    relevance_score: float\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    methodology_details: Optional[Dict]\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two text chunks.\"\"\"\n",
    "        tfidf_matrix = self.tfidf.fit_transform([text1, text2])\n",
    "        return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.3) -> List[str]:\n",
    "        \"\"\"Merge chunks that are semantically similar.\"\"\"\n",
    "        merged_chunks = []\n",
    "        current_chunk = chunks[0]\n",
    "\n",
    "        for next_chunk in chunks[1:]:\n",
    "            similarity = self.get_semantic_similarity(current_chunk, next_chunk)\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk = f\"{current_chunk}\\n{next_chunk}\"\n",
    "            else:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks\n",
    "\n",
    "class AcademicPaperProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str = \"us-central1\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        self.project_id = project_id\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "\n",
    "    def extract_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract metadata from paper header/title section.\"\"\"\n",
    "        prompt = \"\"\"You are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\n",
    "        Focus on accuracy and completeness.\n",
    "\n",
    "        Required fields:\n",
    "        1. Title (exact paper title)\n",
    "        2. Authors (full list with affiliations if available)\n",
    "        3. Publication details:\n",
    "           - Year\n",
    "           - Journal/Conference\n",
    "           - DOI\n",
    "           - Volume/Issue\n",
    "        4. Keywords (if present)\n",
    "        5. Research domain/field\n",
    "\n",
    "        Format the response as a valid Python dictionary.\n",
    "\n",
    "        Text to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(text=text[:3000]),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            return eval(response.text)\n",
    "        except:\n",
    "            return {\"title\": \"Unknown\", \"authors\": [], \"year\": None}\n",
    "\n",
    "    def analyze_chunk(self, chunk: str, metadata: Dict) -> ProcessedChunk:\n",
    "        \"\"\"Analyze chunk content with enhanced agentic understanding.\"\"\"\n",
    "        prompt = \"\"\"You are an expert research analyst. Analyze this academic paper excerpt.\n",
    "\n",
    "        Task: Extract and structure the following components:\n",
    "\n",
    "        1. Core Scientific Content:\n",
    "           - Main findings or theoretical concepts\n",
    "           - Methodologies or approaches\n",
    "           - Evidence supporting claims\n",
    "\n",
    "        2. Practical Applications:\n",
    "           - How this information can be applied\n",
    "\n",
    "        3. Critical Analysis:\n",
    "           - Scientific validity (0-1)\n",
    "           - Practical applicability (0-1)\n",
    "           - Identify limitations or constraints\n",
    "\n",
    "        Format response as a Python dictionary with keys:\n",
    "        {\n",
    "            'text': 'core scientific content',\n",
    "            'use_case': 'practical applications',\n",
    "            'section_type': 'type of section',\n",
    "            'relevance_score': float,\n",
    "            'key_findings': [list of findings],\n",
    "            'citations': [list of referenced papers],\n",
    "            'methodology_details': {dict of methods}\n",
    "        }\n",
    "\n",
    "        Text to analyze:\n",
    "        {chunk}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(chunk=chunk),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = eval(response.text)\n",
    "            return ProcessedChunk(\n",
    "                text=result['text'],\n",
    "                use_case=result['use_case'],\n",
    "                metadata=metadata,\n",
    "                section_type=result['section_type'],\n",
    "                relevance_score=result['relevance_score'],\n",
    "                key_findings=result['key_findings'],\n",
    "                citations=result['citations'],\n",
    "                methodology_details=result['methodology_details']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            return ProcessedChunk(\n",
    "                text=chunk,\n",
    "                use_case=\"\",\n",
    "                metadata=metadata,\n",
    "                section_type=\"unknown\",\n",
    "                relevance_score=0.0,\n",
    "                key_findings=[],\n",
    "                citations=[],\n",
    "                methodology_details=None\n",
    "            )\n",
    "\n",
    "    def agentic_chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Perform intelligent chunking based on semantic meaning.\"\"\"\n",
    "        section_pattern = r'\\n#{1,3}\\s+[A-Z].*?\\n'\n",
    "        sections = re.split(section_pattern, text)\n",
    "\n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            initial_chunks = self.text_splitter.create_documents([section])\n",
    "            chunk_texts = [chunk.page_content for chunk in initial_chunks]\n",
    "            merged_chunks = self.semantic_chunker.merge_similar_chunks(chunk_texts)\n",
    "            chunks.extend(merged_chunks)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[ProcessedChunk]:\n",
    "        \"\"\"Process a PDF file with enhanced chunking.\"\"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            doc = PyPDF2.PdfReader(file)  # Pass the file object, not just the path\n",
    "            text = \"\"\n",
    "            for page in doc.pages:  # Iterate over the pages\n",
    "                text += page.extract_text()  # Use extract_text()\n",
    "\n",
    "        metadata = self.extract_metadata(text)\n",
    "        chunks = self.agentic_chunk_text(text)\n",
    "\n",
    "        processed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            processed_chunk = self.analyze_chunk(chunk, metadata)\n",
    "            if processed_chunk.relevance_score > 0.6:\n",
    "                processed_chunks.append(processed_chunk)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "    def vectorize_chunks(self, chunks: List[ProcessedChunk]) -> List[Dict]:\n",
    "        \"\"\"Enhanced vectorization with semantic understanding.\"\"\"\n",
    "        vectorized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            combined_text = f\"\"\"\n",
    "            Content: {chunk.text}\n",
    "\n",
    "            Key Findings: {' | '.join(chunk.key_findings)}\n",
    "\n",
    "            Practical Applications: {chunk.use_case}\n",
    "\n",
    "            Methodology: {chunk.methodology_details if chunk.methodology_details else 'Not specified'}\n",
    "\n",
    "            Section Type: {chunk.section_type}\n",
    "            \"\"\"\n",
    "\n",
    "            metadata = {\n",
    "                **chunk.metadata,\n",
    "                \"section_type\": chunk.section_type,\n",
    "                \"relevance_score\": chunk.relevance_score,\n",
    "                \"use_case\": chunk.use_case,\n",
    "                \"key_findings\": chunk.key_findings,\n",
    "                \"citations\": chunk.citations,\n",
    "                \"methodology_details\": chunk.methodology_details\n",
    "            }\n",
    "\n",
    "            vectorized_chunks.append({\n",
    "                \"text\": combined_text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "        return vectorized_chunks\n",
    "\n",
    "class ChunkInspector:\n",
    "    def __init__(self, processor: AcademicPaperProcessor):\n",
    "        self.processor = processor\n",
    "        self.console = Console()\n",
    "\n",
    "    def inspect_chunks(self, pdf_path: str, max_chunks: int = 5):\n",
    "        \"\"\"Inspect chunks and embeddings before upload.\"\"\"\n",
    "        processed_chunks = self.processor.process_pdf(pdf_path)\n",
    "        vectors = self.processor.vectorize_chunks(processed_chunks)\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Chunk Processing Summary:[/bold blue]\")\n",
    "        self.console.print(f\"Total chunks extracted: {len(vectors)}\")\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Sample Chunks:[/bold blue]\")\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Chunk #\", style=\"dim\")\n",
    "        table.add_column(\"Text Preview\")\n",
    "        table.add_column(\"Relevance Score\")\n",
    "        table.add_column(\"Section Type\")\n",
    "        table.add_column(\"Key Findings Count\")\n",
    "\n",
    "        for i, vector in enumerate(vectors[:max_chunks]):\n",
    "            metadata = vector['metadata']\n",
    "            table.add_row(\n",
    "                str(i + 1),\n",
    "                vector['text'][:100] + \"...\",\n",
    "                f\"{metadata['relevance_score']:.2f}\",\n",
    "                metadata['section_type'],\n",
    "                str(len(metadata.get('key_findings', [])))\n",
    "            )\n",
    "\n",
    "        self.console.print(table)\n",
    "\n",
    "    def export_to_csv(self, vectors: List[Dict], output_path: str):\n",
    "        \"\"\"Export chunks and metadata to CSV.\"\"\"\n",
    "        records = []\n",
    "        for vector in vectors:\n",
    "            metadata = vector['metadata']\n",
    "            records.append({\n",
    "                'text_preview': vector['text'][:200],\n",
    "                'section_type': metadata['section_type'],\n",
    "                'relevance_score': metadata['relevance_score'],\n",
    "                'key_findings_count': len(metadata.get('key_findings', []))\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        self.console.print(f\"\\n[green]Exported analysis to {output_path}[/green]\")\n",
    "\n",
    "def main():\n",
    "    processor = AcademicPaperProcessor(\n",
    "        project_id=\"athlyze-446917\",\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    inspector = ChunkInspector(processor)\n",
    "\n",
    "    pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "    inspector.inspect_chunks(pdf_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
