{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ijerph-16-04897.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/ijerph-16-04897.pdf\n",
      "Chunks extracted\n",
      "msse-53-1206.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/msse-53-1206.pdf\n",
      "Chunks extracted\n",
      "2102.00836v2.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/2102.00836v2.pdf\n",
      "Chunks extracted\n",
      "fphys-12-791999.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/fphys-12-791999.pdf\n",
      "Chunks extracted\n",
      "fspor-04-949021.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/fspor-04-949021.pdf\n",
      "Chunks extracted\n",
      "jfmk-09-00009.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/jfmk-09-00009.pdf\n",
      "Chunks extracted\n",
      "Combined text has been saved to resistant.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    print(f\"Text extracted from {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove References section (if applicable)\n",
    "    text = re.sub(r'References.*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove headers/footers (example pattern, adjust as needed)\n",
    "    text = re.sub(r'Header text pattern.*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'Footer text pattern.*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove non-alphanumeric characters (if necessary) and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,?!:;\\'\"-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Optional: Convert to lowercase to standardize\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) > chunk_size:\n",
    "            chunks.append(chunk)\n",
    "            chunk = sentence\n",
    "        else:\n",
    "            chunk += \" \" + sentence\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    print(f\"Chunks extracted\")\n",
    "    return chunks\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"{file_name} is under processing...\")\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            raw_text = extract_text_from_pdf(file_path)\n",
    "        else:\n",
    "            continue  # Skip non-supported file types\n",
    "        \n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "        \n",
    "        # Combine all chunks into one text (can save to individual files or a combined file)\n",
    "        combined_text += \"\\n\".join(chunks) + \"\\n\"\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "# Specify the folder containing your research files\n",
    "folder_path = \"resistant_research_papers\"\n",
    "\n",
    "# Process the files in the folder and get the combined cleaned text\n",
    "combined_cleaned_text = process_files_in_folder(folder_path)\n",
    "\n",
    "# Save the combined cleaned and chunked text to a file\n",
    "output_file = \"resistant.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(combined_cleaned_text)\n",
    "\n",
    "print(f\"Combined text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gemini for advanced agentic chunking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Attempting Section based chunking/ seperating\n",
    "\n",
    "    Problems:\n",
    "    \n",
    "        a. Loss of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from agentic_chunker import AgenticChunker\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "# Initialize Gemini client\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.0-pro\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[DEBUG] Attempting to extract text from PDF: {pdf_path}\")\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        print(\"[DEBUG] Text extraction successful.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_prompt(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the agentic chunking prompt for research papers.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Creating the agentic chunking prompt...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI model trained to analyze and extract entire data from scientific research papers.\n",
    "    Your task is to extract and organize the content of the provided paper into the following sections:\n",
    "\n",
    "    1. Objectives: Clearly describe the purpose or aim of the study.\n",
    "    2. Methods: Summarize the methodology, including design, participants, and analysis techniques.\n",
    "    3. Results: Provide key findings or outcomes of the study.\n",
    "    4. Discussion: Highlight the relevance and implications of the findings.\n",
    "    5. Conclusion: Summarize the main takeaways.\n",
    "    6. Practical Applications: Explain how the findings can be applied in real-world contexts.\n",
    "    7. References: Automatically detect and include the references or citations for this research paper.\n",
    "\n",
    "    Include all text from the paper in the response, ensuring no details are omitted. \n",
    "\n",
    "    Here is the content of the research paper:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Prompt successfully created.\")\n",
    "    return prompt\n",
    "\n",
    "def parse_response(response: str) -> dict:\n",
    "    \"\"\"\n",
    "    Dynamically parse the response into structured sections.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Parsing response from the LLM...\")\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for line in response.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"**\") and line.endswith(\":**\"):\n",
    "            # Detect section headers like \"**Objectives:**\"\n",
    "            current_section = line.strip(\"*:\").strip()\n",
    "            sections[current_section] = \"\"\n",
    "        elif current_section:\n",
    "            # Append content to the current section\n",
    "            sections[current_section] += line + \" \"\n",
    "\n",
    "    print(\"[DEBUG] Parsing complete.\")\n",
    "    return {k: v.strip() for k, v in sections.items()}\n",
    "\n",
    "def process_research_paper(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Process a research paper PDF file and extract structured information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Extract text from the PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            print(\"[ERROR] No text extracted. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Step 2: Create the prompt\n",
    "        prompt = create_prompt(text)\n",
    "\n",
    "        # Step 3: Use the LLM to process the prompt\n",
    "        print(\"[DEBUG] Sending prompt to the LLM...\")\n",
    "        message = HumanMessage(content=prompt)\n",
    "        response = llm.invoke([message])\n",
    "        print(f\"[DEBUG] Response received:\\n{response}\")\n",
    "\n",
    "        # Step 4: Parse the response into structured sections\n",
    "        sections = parse_response(response)\n",
    "        print(\"[DEBUG] Structured sections extracted.\")\n",
    "\n",
    "        # Step 5: Display the extracted information\n",
    "        print(\"\\nExtracted Information:\")\n",
    "        for section, content in sections.items():\n",
    "            print(f\"\\n**{section}:**\")\n",
    "            print(content if content else \"No information extracted.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Path to your PDF\n",
    "process_research_paper(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "[DEBUG] Attempting to extract text from PDF: resistant_research_papers/2102.00836v2.pdf\n",
      "[DEBUG] Text extraction successful.\n",
      "[DEBUG] Creating the chunking prompt...\n",
      "[DEBUG] Prompt successfully created.\n",
      "[DEBUG] Sending prompt to the LLM...\n",
      "[DEBUG] Parsing the chunked response...\n",
      "[DEBUG] Parsing complete.\n",
      "[DEBUG] Chunking complete. Total chunks extracted: 1\n",
      "\n",
      "Extracted Chunks:\n",
      "\n",
      "[Chunk 1]:\n",
      "**Why exercise builds muscles: Titin mechanosensing controls skeletal muscle growth under load**\n",
      "Neil Ibata and Eugene M. Terentjev \n",
      "(Dated: May 6, 2021)\n",
      "Muscles sense internally generated and externally applied forces, responding to these in a coordinated hier-\n",
      "archical manner at different timescales. The center of the basic unit of the muscle, the sarcomeric M-band, is\n",
      "perfectly placed to sense the different types of load to which the muscle is subjected. In particular, the kinase\n",
      "domain (TK) of titin located at the M-band is a known candidate for mechanical signaling. Here, we develop\n",
      "the quantitative mathematical model that describes the kinetics of TK-based mechanosensitive signaling, and\n",
      "predicts trophic changes in response to exercise and rehabilitation regimes. First, we build the kinetic model\n",
      "for TK conformational changes under force: opening, phosphorylation, signaling and autoinhibition. We ﬁnd\n",
      "that TK opens as a metastable mechanosensitive switch, which naturally produces a much greater signal after\n",
      "high-load resistance exercise than an equally energetically costly endurance effort. Next, in order for the model\n",
      "to be stable, give coherent predictions, in particular the lag following the onset of an exercise regime, we have\n",
      "to account for the associated kinetics of phosphate (carried by ATP), and for the non-linear dependence of pro-\n",
      "tein synthesis rates on muscle ﬁbre size. We suggest that the latter effect may occur via the steric inhibition\n",
      "of ribosome diffusion through the sieve-like myoﬁlament lattice. The full model yields a steady-state solution\n",
      "(homeostasis) for muscle cross-sectional area and tension, and a quantitatively plausible hypertrophic response\n",
      "to training as well as atrophy following an extended reduction in tension.\n",
      "I. INTRODUCTION\n",
      "Why does exercise build skeletal muscles, whereas long pe-\n",
      "riods of immobility lead to muscle atrophy? The anecdotal\n",
      "evidence is clear, and the sports and rehabilitation medicine\n",
      "community has amassed a large amount of empirical knowl-\n",
      "edge on this topic. But the community has not as yet ad-\n",
      "dressed and understood two key phenomena which underly\n",
      "hypertrophy and atrophy: how does the muscle ‘know’ that it\n",
      "is being exercised (when it is certainly not the tactile sense,\n",
      "processed via the nervous system, that is at play in this), and\n",
      "how does it signal to provoke a morphological response to an\n",
      "increase or a lack of applied load? In some communities, there\n",
      "is a perception that muscle grows after exercise due to its in-\n",
      "ternal repair of micro-damage inﬂicted by the load. However,\n",
      "it is obvious that such an idea cannot be true, for several rea-\n",
      "sons: most of the ‘tissue repair’ occurs by growing connective\n",
      "tissue, while we need an increase of intricately hierarchical\n",
      "myoﬁlament structure, also this concept will not account for\n",
      "atrophy developing in microgravity or after extended bedrest.\n",
      "Here we develop a quantitative theoretical model which\n",
      "seeks to explain both of these processes. In order to be useful,\n",
      "the model must build on the relevant knowledge accumulated\n",
      "from studies of the anatomy and physiology of muscles, as\n",
      "well as the biological physics of molecular interactions and\n",
      "forces.\n",
      "Muscles, their constituent cells, and the structure of their\n",
      "molecular ﬁlament mesh must respond mechanosensitively –\n",
      "i.e.in a manner which depends on the changes in the magni-\n",
      "tude of the forces and stresses that arise during the contraction\n",
      "and extension of the muscle – at many different timescales.\n",
      "At the fastest time scales (tens or hundreds of milliseconds),\n",
      "skeletal muscles can produce near maximal force for jump-\n",
      "ing or for the ﬁght-or-ﬂight response. Most muscles also go\n",
      "through cycles of shortening and lengthening with a period\n",
      "of the order of a second in the vast majority of sprint or en-\n",
      "durance exercise (running, climbing, etc.) At a much longer\n",
      "timescale of many days, a muscle must also be able to mea-\n",
      "sure changes in its overall use in order to effect adaptive mus-\n",
      "cle hypertrophy/atrophy – ultimately helping to prevent injury\n",
      "on the scale of months and years.\n",
      "How the muscle cell keeps track of the history of its load\n",
      "and stress inputs within a number of intracellular output sig-\n",
      "nals (which then go on to stimulate or inhibit muscle protein\n",
      "synthesis), is inherently an incredibly complex biochemical\n",
      "question. With the help of recent theoretical insights into the\n",
      "folding and unfolding rates of mechanosensor proteins un-\n",
      "der force, we hope to gain insights into the ﬁrst part of this\n",
      "puzzle for the speciﬁc case of muscle hypertrophy. To make\n",
      "progress, we use a simple model for force-induced transitions\n",
      "between the different conformations of the titin kinase (TK)\n",
      "mechanosensor. If the conformational change helps create\n",
      "an intracellular signal, we can model the signal’s strength in\n",
      "terms of the duration and intensity of the mechanical inputs\n",
      "(external force on the TK domain in our case).\n",
      "Force chain\n",
      "The individual sub-cellular, cellular and super-cellular\n",
      "components of a muscle act in concert to scale up a vast num-\n",
      "ber of molecular force-generating events into a macroscopic\n",
      "force. The hierarchical structure of the muscle (see Fig. 1)\n",
      "allows the macroscopic and microscopic responses to mirror\n",
      "each other [10].\n",
      "The sarcomere is the elementary unit of the muscle cell\n",
      "and the basic building block of the sliding ﬁlament hypoth-\n",
      "esis [11, 12]. Its regular and conserved structure, sketched in\n",
      "Fig. 2 for the vertebrate striated muscle, allows for a series\n",
      "transmission of tension over the whole length of the muscle.\n",
      "In vertebrates, six titin molecules are wrapped around each\n",
      "thick ﬁlament [13, 14] on either side of the midpoint of the\n",
      "sarcomere: the M-line.\n",
      "During active muscle contraction, myosin heads (motors)\n",
      "bind to actin and ‘walk’ in an ATP-controlled sequence of\n",
      "steps [15] along the thin ﬁlaments. When a resistance is ap-\n",
      "plied, the myosin motor exerts a force against it. During slow\n",
      "resistance training in both concentric and eccentric motions,\n",
      "tension is passed along the sarcomere primarily through the\n",
      "thin ﬁlament, myosin heads [16, 17], the thick ﬁlament, and\n",
      "into the cross-bridge region of the sarcomere where thick ﬁla-\n",
      "ments are crosslinked with their associated M-band proteins.\n",
      "The load in each of the sarcomere components ultimately\n",
      "depends on the relative compliance of elements. The relative\n",
      "load on the thick ﬁlament and the M-band segments of titin\n",
      "when the ﬁlament is either under internal (contracting) or ex-\n",
      "ternal (extending) load is discussed in Supplementary A.4. It\n",
      "is well-known that titin is under load when the sarcomere is\n",
      "extended [18, 19]. Recent X-ray diffraction experiments [20]\n",
      "suggest that that the thick ﬁlament may be more compliant\n",
      "than originally thought; if so, M-line titin is likely substan-\n",
      "tially extended and loaded titin when the muscle actively gen-\n",
      "erates force. Others disagree [21] and attribute the change in\n",
      "line spacing in diffusion experiments to a mechanosensitive\n",
      "activation of the entire thick ﬁlament at low forces. Either\n",
      "way, M-band titin is under some tension during active muscle\n",
      "contraction. This situation is sketched in Fig. 2.\n",
      "We estimate the force in each ﬁlament both macroscopi-\n",
      "cally and microscopically (see the full discussion in Supple-\n",
      "mentary Part A.2 and A.3). We divide the force in the entire\n",
      "muscle by the number of active myoﬁlaments (see Fig. 1) to\n",
      "ﬁnd a large variation in force per ﬁlament in untrained individ-\n",
      "uals (150 500pN) [7]. Muscle ﬁbre neuronal and molecular\n",
      "activation increases with training [8, 9], so the higher forces\n",
      "are more likely representative of ﬁlament forces in trained in-\n",
      "dividuals. The maximum ﬁlament forces extrapolated from\n",
      "X-ray diffraction studies [22] are higher at  600pN, possi-\n",
      "bly because the actin only partially binds to myosin in normal\n",
      "contractions, maximum forces do not last very long, and be-\n",
      "cause the muscle does not coordinate perfectly as a whole. In\n",
      "Supplementary Part A.4, we graphically ﬁnd an approximate\n",
      "relation between titin and thick ﬁlament force. In particular, it\n",
      "suggests that the force per titin be close to 25pN at the maxi-\n",
      "mum voluntary contraction force.\n",
      "In Section A.1 of the Supplementary Material, we discuss\n",
      "different candidates of mechanosensor proteins in the sar-\n",
      "comere, and highlight the reasons why titin kinase is a par-\n",
      "ticularly good candidate for this role, and why we have not\n",
      "considered some of these other candidates here.\n",
      "TK is a mechanosensor of the ‘second kind’\n",
      "Cells sense and respond to the mechanical properties of\n",
      "their environment using two main classes of force receptors.3\n",
      "The ﬁrst type of mechanosensor responds immediately under\n",
      "force [23, 24]. Mechanosensitive ion channels are the archety-\n",
      "pal example of such a sensor and have been proposed to play\n",
      "a role in tactile signaling (transforming a mechanical signal\n",
      "into chemical) [23, 25]. However, the ions which they use\n",
      "in signaling are rapidly depleted, making it difﬁcult for these\n",
      "sensors\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from agentic_chunker import AgenticChunker\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "# Initialize Gemini client\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.0-pro\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[DEBUG] Attempting to extract text from PDF: {pdf_path}\")\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        print(\"[DEBUG] Text extraction successful.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_chunking_prompt(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for chunking the text into meaningful parts.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Creating the chunking prompt...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant skilled in analyzing and chunking research paper text. Your task is to break the text into meaningful chunks. \n",
    "    Ensure the chunks has the original text from the paper. NO summarization; focus on grouping related sentences together.\n",
    "    Make sure that the last chunk holds the reference to this research paper.\n",
    "    Here is the content of the research paper to chunk, do not create your own text, rather grough the text below together which have sementic similarities:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Prompt successfully created.\")\n",
    "    return prompt\n",
    "\n",
    "def parse_chunking_response(response: str) -> list:\n",
    "    \"\"\"\n",
    "    Parse the chunking response into a list of chunks.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Parsing the chunked response...\")\n",
    "    chunks = response.split(\"\\n\\n\")\n",
    "    parsed_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    print(\"[DEBUG] Parsing complete.\")\n",
    "    return parsed_chunks\n",
    "\n",
    "def process_research_paper_with_chunking(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Process a research paper PDF file and chunk its content into meaningful parts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Extract text from the PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            print(\"[ERROR] No text extracted. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Step 2: Create the chunking prompt\n",
    "        prompt = create_chunking_prompt(text)\n",
    "\n",
    "        # Step 3: Use the LLM to process the prompt\n",
    "        print(\"[DEBUG] Sending prompt to the LLM...\")\n",
    "        message = HumanMessage(content=prompt)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        # Step 4: Parse the response into chunks\n",
    "        chunks = parse_chunking_response(response)\n",
    "        print(\"[DEBUG] Chunking complete. Total chunks extracted:\", len(chunks))\n",
    "\n",
    "        # Step 5: Display the extracted chunks\n",
    "        print(\"\\nExtracted Chunks:\")\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\n[Chunk {i}]:\")\n",
    "            print(chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Path to your PDF\n",
    "process_research_paper_with_chunking(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AthlyzeRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
