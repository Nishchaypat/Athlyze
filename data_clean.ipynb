{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ijerph-16-04897.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/ijerph-16-04897.pdf\n",
      "Chunks extracted\n",
      "msse-53-1206.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/msse-53-1206.pdf\n",
      "Chunks extracted\n",
      "2102.00836v2.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/2102.00836v2.pdf\n",
      "Chunks extracted\n",
      "fphys-12-791999.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/fphys-12-791999.pdf\n",
      "Chunks extracted\n",
      "fspor-04-949021.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/fspor-04-949021.pdf\n",
      "Chunks extracted\n",
      "jfmk-09-00009.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/jfmk-09-00009.pdf\n",
      "Chunks extracted\n",
      "Combined text has been saved to resistant.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    print(f\"Text extracted from {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove References section (if applicable)\n",
    "    text = re.sub(r'References.*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove headers/footers (example pattern, adjust as needed)\n",
    "    text = re.sub(r'Header text pattern.*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'Footer text pattern.*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove non-alphanumeric characters (if necessary) and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,?!:;\\'\"-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Optional: Convert to lowercase to standardize\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) > chunk_size:\n",
    "            chunks.append(chunk)\n",
    "            chunk = sentence\n",
    "        else:\n",
    "            chunk += \" \" + sentence\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    print(f\"Chunks extracted\")\n",
    "    return chunks\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"{file_name} is under processing...\")\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            raw_text = extract_text_from_pdf(file_path)\n",
    "        else:\n",
    "            continue  # Skip non-supported file types\n",
    "        \n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "        \n",
    "        # Combine all chunks into one text (can save to individual files or a combined file)\n",
    "        combined_text += \"\\n\".join(chunks) + \"\\n\"\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "# Specify the folder containing your research files\n",
    "folder_path = \"resistant_research_papers\"\n",
    "\n",
    "# Process the files in the folder and get the combined cleaned text\n",
    "combined_cleaned_text = process_files_in_folder(folder_path)\n",
    "\n",
    "# Save the combined cleaned and chunked text to a file\n",
    "output_file = \"resistant.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(combined_cleaned_text)\n",
    "\n",
    "print(f\"Combined text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AthlyzeRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
