{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ijerph-16-04897.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/ijerph-16-04897.pdf\n",
      "Chunks extracted\n",
      "msse-53-1206.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/msse-53-1206.pdf\n",
      "Chunks extracted\n",
      "2102.00836v2.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/2102.00836v2.pdf\n",
      "Chunks extracted\n",
      "fphys-12-791999.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/fphys-12-791999.pdf\n",
      "Chunks extracted\n",
      "fspor-04-949021.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/fspor-04-949021.pdf\n",
      "Chunks extracted\n",
      "jfmk-09-00009.pdf is under processing...\n",
      "Text extracted from resistant_research_papers/jfmk-09-00009.pdf\n",
      "Chunks extracted\n",
      "Combined text has been saved to resistant.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    print(f\"Text extracted from {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove References section (if applicable)\n",
    "    text = re.sub(r'References.*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove headers/footers (example pattern, adjust as needed)\n",
    "    text = re.sub(r'Header text pattern.*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'Footer text pattern.*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove non-alphanumeric characters (if necessary) and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,?!:;\\'\"-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Optional: Convert to lowercase to standardize\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) > chunk_size:\n",
    "            chunks.append(chunk)\n",
    "            chunk = sentence\n",
    "        else:\n",
    "            chunk += \" \" + sentence\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    print(f\"Chunks extracted\")\n",
    "    return chunks\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"{file_name} is under processing...\")\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            raw_text = extract_text_from_pdf(file_path)\n",
    "        else:\n",
    "            continue  # Skip non-supported file types\n",
    "        \n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "        \n",
    "        # Combine all chunks into one text (can save to individual files or a combined file)\n",
    "        combined_text += \"\\n\".join(chunks) + \"\\n\"\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "# Specify the folder containing your research files\n",
    "folder_path = \"resistant_research_papers\"\n",
    "\n",
    "# Process the files in the folder and get the combined cleaned text\n",
    "combined_cleaned_text = process_files_in_folder(folder_path)\n",
    "\n",
    "# Save the combined cleaned and chunked text to a file\n",
    "output_file = \"resistant.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(combined_cleaned_text)\n",
    "\n",
    "print(f\"Combined text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gemini for advanced agentic chunking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Attempting Section based chunking/ seperating\n",
    "\n",
    "    Problems:\n",
    "    \n",
    "        a. Loss of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from agentic_chunker import AgenticChunker\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "# Initialize Gemini client\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.0-pro\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[DEBUG] Attempting to extract text from PDF: {pdf_path}\")\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        print(\"[DEBUG] Text extraction successful.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_prompt(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the agentic chunking prompt for research papers.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Creating the agentic chunking prompt...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI model trained to analyze and extract entire data from scientific research papers.\n",
    "    Your task is to extract and organize the content of the provided paper into the following sections:\n",
    "\n",
    "    1. Objectives: Clearly describe the purpose or aim of the study.\n",
    "    2. Methods: Summarize the methodology, including design, participants, and analysis techniques.\n",
    "    3. Results: Provide key findings or outcomes of the study.\n",
    "    4. Discussion: Highlight the relevance and implications of the findings.\n",
    "    5. Conclusion: Summarize the main takeaways.\n",
    "    6. Practical Applications: Explain how the findings can be applied in real-world contexts.\n",
    "    7. References: Automatically detect and include the references or citations for this research paper.\n",
    "\n",
    "    Include all text from the paper in the response, ensuring no details are omitted. \n",
    "\n",
    "    Here is the content of the research paper:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Prompt successfully created.\")\n",
    "    return prompt\n",
    "\n",
    "def parse_response(response: str) -> dict:\n",
    "    \"\"\"\n",
    "    Dynamically parse the response into structured sections.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Parsing response from the LLM...\")\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for line in response.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"**\") and line.endswith(\":**\"):\n",
    "            # Detect section headers like \"**Objectives:**\"\n",
    "            current_section = line.strip(\"*:\").strip()\n",
    "            sections[current_section] = \"\"\n",
    "        elif current_section:\n",
    "            # Append content to the current section\n",
    "            sections[current_section] += line + \" \"\n",
    "\n",
    "    print(\"[DEBUG] Parsing complete.\")\n",
    "    return {k: v.strip() for k, v in sections.items()}\n",
    "\n",
    "def process_research_paper(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Process a research paper PDF file and extract structured information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Extract text from the PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            print(\"[ERROR] No text extracted. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Step 2: Create the prompt\n",
    "        prompt = create_prompt(text)\n",
    "\n",
    "        # Step 3: Use the LLM to process the prompt\n",
    "        print(\"[DEBUG] Sending prompt to the LLM...\")\n",
    "        message = HumanMessage(content=prompt)\n",
    "        response = llm.invoke([message])\n",
    "        print(f\"[DEBUG] Response received:\\n{response}\")\n",
    "\n",
    "        # Step 4: Parse the response into structured sections\n",
    "        sections = parse_response(response)\n",
    "        print(\"[DEBUG] Structured sections extracted.\")\n",
    "\n",
    "        # Step 5: Display the extracted information\n",
    "        print(\"\\nExtracted Information:\")\n",
    "        for section, content in sections.items():\n",
    "            print(f\"\\n**{section}:**\")\n",
    "            print(content if content else \"No information extracted.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Path to your PDF\n",
    "process_research_paper(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "[DEBUG] Attempting to extract text from PDF: resistant_research_papers/2102.00836v2.pdf\n",
      "[DEBUG] Text extraction successful.\n",
      "[DEBUG] Creating the chunking prompt...\n",
      "[DEBUG] Prompt successfully created.\n",
      "[DEBUG] Sending prompt to the LLM...\n",
      "[DEBUG] Response received:\n",
      "**Chunk 1:**\n",
      "- Introduces the topic of muscle growth and hypertrophy\n",
      "- Explains the role of titin mechanosensing in muscle growth\n",
      "- Provides an overview of the model developed by the authors\n",
      "\n",
      "**Chunk 2:**\n",
      "- Describes the structure and function of titin kinase (TK)\n",
      "- Explains how TK opens under force and can be phosphorylated\n",
      "- Introduces the concept of a metastable mechanosensitive switch\n",
      "\n",
      "**Chunk 3:**\n",
      "- Discusses the role of signaling molecules in muscle growth\n",
      "- Explains how the model incorporates the activation of signaling molecules\n",
      "- Introduces the concept of ribosome biogenesis and its role in muscle growth\n",
      "\n",
      "**Chunk 4:**\n",
      "- Explains how the model incorporates the feedback loop between muscle growth and ribosome biogenesis\n",
      "- Discusses the role of force feedback in muscle growth and atrophy\n",
      "\n",
      "**Chunk 5:**\n",
      "- Presents the results of the model simulations\n",
      "- Shows how the model can reproduce experimental data on muscle growth and atrophy\n",
      "- Discusses the implications of the model for understanding muscle growth and hypertrophy\n",
      "[DEBUG] Parsing the chunked response...\n",
      "[DEBUG] Parsing complete.\n",
      "[DEBUG] Chunking complete. Total chunks extracted: 5\n",
      "\n",
      "Extracted Chunks:\n",
      "\n",
      "[Chunk 1]:\n",
      "**Chunk 1:**\n",
      "- Introduces the topic of muscle growth and hypertrophy\n",
      "- Explains the role of titin mechanosensing in muscle growth\n",
      "- Provides an overview of the model developed by the authors\n",
      "\n",
      "[Chunk 2]:\n",
      "**Chunk 2:**\n",
      "- Describes the structure and function of titin kinase (TK)\n",
      "- Explains how TK opens under force and can be phosphorylated\n",
      "- Introduces the concept of a metastable mechanosensitive switch\n",
      "\n",
      "[Chunk 3]:\n",
      "**Chunk 3:**\n",
      "- Discusses the role of signaling molecules in muscle growth\n",
      "- Explains how the model incorporates the activation of signaling molecules\n",
      "- Introduces the concept of ribosome biogenesis and its role in muscle growth\n",
      "\n",
      "[Chunk 4]:\n",
      "**Chunk 4:**\n",
      "- Explains how the model incorporates the feedback loop between muscle growth and ribosome biogenesis\n",
      "- Discusses the role of force feedback in muscle growth and atrophy\n",
      "\n",
      "[Chunk 5]:\n",
      "**Chunk 5:**\n",
      "- Presents the results of the model simulations\n",
      "- Shows how the model can reproduce experimental data on muscle growth and atrophy\n",
      "- Discusses the implications of the model for understanding muscle growth and hypertrophy\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from agentic_chunker import AgenticChunker\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "# Initialize Gemini client\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.0-pro\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[DEBUG] Attempting to extract text from PDF: {pdf_path}\")\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        print(\"[DEBUG] Text extraction successful.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_chunking_prompt(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for chunking the text into meaningful parts.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Creating the chunking prompt...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant skilled in analyzing and chunking research paper text. Your task is to break the text into meaningful chunks. \n",
    "    Ensure the chunks has the original text from the paper. NO summarization; focus on grouping related sentences together.\n",
    "    Remove any reference number added in the sentences.\n",
    "    Here is the content of the research paper to chunk, do not create your own text, rather grough the text below together which have sementic similarities:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Prompt successfully created.\")\n",
    "    return prompt\n",
    "\n",
    "def parse_chunking_response(response: str) -> list:\n",
    "    \"\"\"\n",
    "    Parse the chunking response into a list of chunks.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Parsing the chunked response...\")\n",
    "    chunks = response.split(\"\\n\\n\")\n",
    "    parsed_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    print(\"[DEBUG] Parsing complete.\")\n",
    "    return parsed_chunks\n",
    "\n",
    "def process_research_paper_with_chunking(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Process a research paper PDF file and chunk its content into meaningful parts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Extract text from the PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            print(\"[ERROR] No text extracted. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Step 2: Create the chunking prompt\n",
    "        prompt = create_chunking_prompt(text)\n",
    "\n",
    "        # Step 3: Use the LLM to process the prompt\n",
    "        print(\"[DEBUG] Sending prompt to the LLM...\")\n",
    "        message = HumanMessage(content=prompt)\n",
    "        response = llm.invoke([message])\n",
    "        print(f\"[DEBUG] Response received:\\n{response}\")\n",
    "\n",
    "        # Step 4: Parse the response into chunks\n",
    "        chunks = parse_chunking_response(response)\n",
    "        print(\"[DEBUG] Chunking complete. Total chunks extracted:\", len(chunks))\n",
    "\n",
    "        # Step 5: Display the extracted chunks\n",
    "        print(\"\\nExtracted Chunks:\")\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\n[Chunk {i}]:\")\n",
    "            print(chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Path to your PDF\n",
    "process_research_paper_with_chunking(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AthlyzeRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
