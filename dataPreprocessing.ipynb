{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agentic Segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# Get requirements\n",
    "\n",
    "gemini = os.getenv(\"GEMINI_API_KEY\")\n",
    "location = os.getenv(\"location\")\n",
    "location_processor = os.getenv(\"location_processor\")\n",
    "project_id = os.getenv(\"project_id\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    model = GoogleGenerativeAI(\n",
    "        model=\"gemini-1.0-pro\",\n",
    "        google_api_key=gemini,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def clean_response(response_str):\n",
    "    \"\"\"\n",
    "    Cleans the response string by removing the code block markers and then attempts to convert it to JSON.\n",
    "    \"\"\"\n",
    "    # Remove the code block markers (start and end)\n",
    "    response_str = re.sub(r'^```json\\n', '', response_str)\n",
    "    response_str = re.sub(r'```$', '', response_str).strip()\n",
    "    \n",
    "    # Attempt to parse the cleaned string into a JSON object\n",
    "    try:\n",
    "        response_json = json.loads(response_str)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing the response as JSON, response was:\", response_str)\n",
    "        return {\"findings\": [], \"metadata\": {}}\n",
    "\n",
    "def chunk_and_clean_text(model, raw_text):\n",
    "    prompt = \"\"\"\n",
    "    You are a highly capable AI model tasked with cleaning and chunking the provided text.\n",
    "    Please return the response in JSON format with two keys:\n",
    "    - \"findings\": A list of valid claims or facts related to muscle training, nutrition, gym, biology, etc.\n",
    "    - \"metadata\": A dictionary containing the \"title\" key with the paper's title.\n",
    "    Here is the input text: {raw_text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get response from Gemini model (in string format)\n",
    "    response_str = model(prompt.format(raw_text=raw_text))\n",
    "        \n",
    "    # Clean and parse the response string into a JSON object\n",
    "    response_json = clean_response(response_str)\n",
    "    \n",
    "    findings = response_json.get(\"findings\", [])\n",
    "    metadata = response_json.get(\"metadata\", {})\n",
    "    \n",
    "    return {\"findings\": findings, \"metadata\": metadata}\n",
    "\n",
    "def process_pages(pages):\n",
    "    model = initialize_model()\n",
    "    full_response = {\"findings\": [], \"metadata\": {}}\n",
    "    \n",
    "    for page in pages:\n",
    "        print(f\"Processing page {pages.index(page) + 1}...\")\n",
    "        response = chunk_and_clean_text(model, page)\n",
    "        \n",
    "        # Print the response for debugging purposes\n",
    "        print(\"Response:\", response)  # Print the response to verify it's in the correct format\n",
    "        \n",
    "        # Merge findings from the response\n",
    "        if isinstance(response, dict):\n",
    "            # Append findings to full_response['findings']\n",
    "            full_response[\"findings\"].extend(response.get(\"findings\", []))\n",
    "            \n",
    "            # Merge metadata if it's not already set\n",
    "            if not full_response[\"metadata\"]:\n",
    "                full_response[\"metadata\"] = response.get(\"metadata\", {})\n",
    "        else:\n",
    "            print(\"Response is not in the expected format:\", response)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Extract text from PDF using PyPDF2\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages = []\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            pages.append(page_text)\n",
    "    return pages\n",
    "\n",
    "\n",
    "# Path to your PDF file\n",
    "file_path = \"nutrition_research_papers/jmir-2023-1-e37667.pdf\"\n",
    "# Extract text from the PDF using PyPDF2\n",
    "print(\"Extracting text from PDF using PyPDF2...\")\n",
    "pages = extract_text_from_pdf(file_path)\n",
    "print(len(pages))\n",
    "# Perform chunking and cleaning\n",
    "print(\"Cleaning and chunking text from each page...\")\n",
    "final_response = process_pages(pages)\n",
    "\n",
    "# Output the final response\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above to all the files in the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 4 findings: 12\n",
      "Processing page 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 5 findings: 15\n",
      "Processing page 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 6 findings: 14\n",
      "Processing page 7...\n",
      "Page 7 findings: 14\n",
      "Processing page 8...\n",
      "Page 8 findings: 15\n",
      "Processing page 9...\n",
      "Page 9 findings: 16\n",
      "Processing page 10...\n",
      "Page 10 findings: 11\n",
      "Processing page 11...\n",
      "Page 11 findings: 14\n",
      "Processing page 12...\n",
      "Page 12 findings: 25\n",
      "Processing page 13...\n",
      "Page 13 findings: 28\n",
      "Processing page 14...\n",
      "Page 14 findings: 17\n",
      "Processing page 15...\n",
      "Page 15 findings: 35\n",
      "Processing page 16...\n",
      "Page 16 findings: 40\n",
      "Processing page 17...\n",
      "Page 17 findings: 5\n",
      "Processing page 18...\n",
      "Page 18 findings: 17\n",
      "Processing page 19...\n",
      "Page 19 findings: 13\n",
      "Processing page 20...\n",
      "Page 20 findings: 13\n",
      "Processing page 21...\n",
      "Page 21 findings: 10\n",
      "Processing page 22...\n",
      "Page 22 findings: 9\n",
      "Processing page 23...\n",
      "Page 23 findings: 11\n",
      "Processing page 24...\n",
      "Page 24 findings: 8\n",
      "Processing page 25...\n",
      "Page 25 findings: 11\n",
      "Processing page 26...\n",
      "Page 26 findings: 12\n",
      "Processing page 27...\n",
      "Page 27 findings: 11\n",
      "Processing page 28...\n",
      "Page 28 findings: 8\n",
      "Finished processing nutrients-13-01763.pdf. Metadata: {'title': 'A Novel Personalized Systems Nutrition Program Improves Dietary Patterns, Lifestyle Behaviors and Health-Related Outcomes: Results from the Habit Study'}\n",
      "Total findings so far: 4229\n",
      "\n",
      "Processing complete.\n",
      "Failed to process 0 files: []\n",
      "\n",
      "Results saved to aggregated_results.json.\n",
      "Total findings: 4229\n",
      "Metadata collected for 19 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# Initialize the Gemini Pro model\n",
    "def initialize_model():\n",
    "    try:\n",
    "        gemini = os.getenv(\"GEMINI_API_KEY\")\n",
    "        model = GoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            google_api_key=gemini,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        print(\"Model initialized successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Clean the response from Gemini Pro\n",
    "def clean_response(response_str):\n",
    "    response_str = re.sub(r'^```json\\n', '', response_str)\n",
    "    response_str = re.sub(r'```$', '', response_str).strip()\n",
    "    try:\n",
    "        return json.loads(response_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing response as JSON: {response_str[:500]}\")  # Log first 500 characters\n",
    "        return {\"findings\": [], \"metadata\": {}}\n",
    "\n",
    "# Chunk text and send it to the Gemini Pro model\n",
    "def chunk_and_clean_text(model, raw_text):\n",
    "    max_length = 3000  # Adjust based on Gemini Pro's input limit\n",
    "    truncated_text = raw_text[:max_length]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly capable AI model tasked with cleaning and chunking the provided text.\n",
    "    Please return the response in JSON format with two keys:\n",
    "    - \"findings\": A list of valid claims or facts related to muscle training, nutrition, gym, biology, etc.\n",
    "    - \"metadata\": A dictionary containing the \"title\" key with the paper's title.\n",
    "    Here is the input text: {truncated_text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_str = model(prompt)\n",
    "        return clean_response(response_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text chunk: {e}\")\n",
    "        return {\"findings\": [], \"metadata\": {}}\n",
    "\n",
    "# Process each page of a PDF\n",
    "def process_pages(model, pages):\n",
    "    combined_response = {\"findings\": [], \"metadata\": {}}\n",
    "    for page_number, page in enumerate(pages, 1):\n",
    "        print(f\"Processing page {page_number}...\")\n",
    "        response = chunk_and_clean_text(model, page)\n",
    "        combined_response[\"findings\"].extend(response.get(\"findings\", []))\n",
    "        if not combined_response[\"metadata\"]:\n",
    "            combined_response[\"metadata\"] = response.get(\"metadata\", {})\n",
    "        print(f\"Page {page_number} findings: {len(response.get('findings', []))}\")\n",
    "    return combined_response\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            return [page.extract_text() for page in pdf_reader.pages]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process all PDFs in the specified folder\n",
    "def process_folder(folder_path):\n",
    "    model = initialize_model()\n",
    "    aggregated_results = {\"findings\": [], \"metadata\": []}\n",
    "    failed_files = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            print(f\"\\nProcessing file: {file_name}\")\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            pages = extract_text_from_pdf(file_path)\n",
    "            \n",
    "            if not pages:\n",
    "                print(f\"Skipping {file_name}: No readable pages found.\")\n",
    "                failed_files.append(file_name)\n",
    "                continue\n",
    "\n",
    "            file_response = process_pages(model, pages)\n",
    "            aggregated_results[\"findings\"].extend(file_response.get(\"findings\", []))\n",
    "            aggregated_results[\"metadata\"].append({\n",
    "                \"file_name\": file_name,\n",
    "                **file_response.get(\"metadata\", {})\n",
    "            })\n",
    "            print(f\"Finished processing {file_name}. Metadata: {file_response.get('metadata', {})}\")\n",
    "            print(f\"Total findings so far: {len(aggregated_results['findings'])}\")\n",
    "\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Failed to process {len(failed_files)} files: {failed_files}\")\n",
    "    return aggregated_results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"nutrition_research_papers\"  # Folder containing PDF files\n",
    "    print(\"Starting processing for folder:\", folder_path)\n",
    "\n",
    "    final_results = process_folder(folder_path)\n",
    "\n",
    "    # Save the aggregated results to a JSON file\n",
    "    output_file_path = \"aggregated_results.json\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        json.dump(final_results, output_file, indent=4)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_file_path}.\")\n",
    "    print(f\"Total findings: {len(final_results['findings'])}\")\n",
    "    print(f\"Metadata collected for {len(final_results['metadata'])} files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering the embedding for all the files and creating the vector db info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "aiplatform_project = os.getenv(\"PROJECT_ID\")\n",
    "aiplatform_location = os.getenv(\"LOCATION_ID\")\n",
    "processor_id = os.getenv(\"PROCESSOR_ID\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"preprocessing_credentials.json\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=aiplatform_project, location=aiplatform_location)\n",
    "\n",
    "# Function to initialize Gemini model\n",
    "def initialize_model():\n",
    "    print(\"Initializing Gemini model...\")\n",
    "    model = GoogleGenerativeAI(\n",
    "        model=\"gemini-1.0-pro\",\n",
    "        google_api_key=gemini_api_key,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"Gemini model initialized successfully.\")\n",
    "    return model\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(text):\n",
    "    try:\n",
    "        if not text.strip():\n",
    "            print(f\"Skipped embedding: Text is empty or whitespace.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Generating embedding for text: {text[:50]}...\")\n",
    "        model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "        input_obj = TextEmbeddingInput(text=text)\n",
    "        embeddings = model.get_embeddings([input_obj])\n",
    "        print(\"Embedding generated successfully.\")\n",
    "        return embeddings[0].values\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {text[:50]}... Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean Gemini response\n",
    "def clean_response(response_str):\n",
    "    print(\"Cleaning Gemini response...\")\n",
    "    response_str = re.sub(r'^```json\\n', '', response_str)\n",
    "    response_str = re.sub(r'```$', '', response_str).strip()\n",
    "\n",
    "    try:\n",
    "        response_json = json.loads(response_str)\n",
    "        print(\"Response cleaned and converted to JSON.\")\n",
    "        return response_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing the response as JSON, response was:\", response_str)\n",
    "        return {\"findings\": [], \"metadata\": {}}\n",
    "\n",
    "# Function to process text with Gemini\n",
    "def chunk_and_clean_text(model, raw_text):\n",
    "    prompt = \"\"\"\n",
    "    You are a highly capable AI model tasked with cleaning and chunking the provided text.\n",
    "    Please return the response in JSON format with two keys:\n",
    "    - \"findings\": A list of valid claims or facts related to muscle training, nutrition, gym, biology, etc.\n",
    "    - \"metadata\": A dictionary containing the \"title\" key with the paper's title.\n",
    "    Here is the input text: {raw_text}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing text chunk: {raw_text[:100]}...\")\n",
    "    response_str = model(prompt.format(raw_text=raw_text))\n",
    "    response_json = clean_response(response_str)\n",
    "\n",
    "    findings = response_json.get(\"findings\", [])\n",
    "    metadata = response_json.get(\"metadata\", {})\n",
    "    print(f\"Processed text with findings: {len(findings)} items.\")\n",
    "    return {\"findings\": findings, \"metadata\": metadata}\n",
    "\n",
    "# Function to process pages of a PDF\n",
    "def process_pages(model, pages):\n",
    "    full_response = {}\n",
    "    print(\"Starting to process PDF pages...\")\n",
    "\n",
    "    for idx, page in enumerate(pages):\n",
    "        print(f\"Processing page {idx + 1}...\")\n",
    "        response = chunk_and_clean_text(model, page)\n",
    "\n",
    "        title = response[\"metadata\"].get(\"title\", f\"Untitled Page {idx + 1}\")\n",
    "        if title not in full_response:\n",
    "            full_response[title] = []\n",
    "        full_response[title].extend(response[\"findings\"])\n",
    "    print(\"All pages processed.\")\n",
    "    return full_response\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    print(f\"Extracting text from PDF: {file_path}...\")\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages = [page.extract_text() for page in pdf_reader.pages]\n",
    "    print(f\"Extracted {len(pages)} pages from PDF.\")\n",
    "    return pages\n",
    "\n",
    "# Function to process findings with embeddings\n",
    "def process_findings_with_embeddings(findings_by_title):\n",
    "    processed = {}\n",
    "    print(\"Processing findings with embeddings...\")\n",
    "\n",
    "    for title, findings in findings_by_title.items():\n",
    "        print(f\"Processing title: {title} with {len(findings)} findings...\")\n",
    "        embeddings_list = []\n",
    "\n",
    "        for finding in findings:\n",
    "            embedding = generate_embeddings(finding)\n",
    "            if embedding:\n",
    "                embeddings_list.append({\"text\": finding, \"embedding\": embedding})\n",
    "            else:\n",
    "                print(f\"Embedding failed for finding: '{finding}' under title: '{title}'\")\n",
    "\n",
    "        processed[title] = embeddings_list\n",
    "        print(f\"Processed title: {title} with {len(embeddings_list)} embeddings.\")\n",
    "    return processed\n",
    "\n",
    "# Main pipeline function\n",
    "def process_folder(folder_path):\n",
    "    model = initialize_model()\n",
    "    total_data = {\"processed\": {}, \"completed_files\": []}\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        try:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if file_name in total_data[\"completed_files\"]:\n",
    "                print(f\"Skipping already completed file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing file: {file_name}...\")\n",
    "            pages = extract_text_from_pdf(file_path)\n",
    "            findings_by_title = process_pages(model, pages)\n",
    "            embeddings_data = process_findings_with_embeddings(findings_by_title)\n",
    "\n",
    "            total_data[\"processed\"].update(embeddings_data)\n",
    "            total_data[\"completed_files\"].append(file_name)\n",
    "            print(f\"File {file_name} processed successfully.\")\n",
    "\n",
    "            # Save progress after each file\n",
    "            with open(\"progress_with_embeddings.json\", \"w\") as f:\n",
    "                json.dump(total_data, f, indent=4)\n",
    "            print(\"Progress saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    return total_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"nutrition_research_papers\"\n",
    "    print(\"Starting pipeline...\")\n",
    "    start_time = datetime.now()\n",
    "    result = process_folder(folder_path)\n",
    "    print(f\"Pipeline completed in {datetime.now() - start_time}.\")\n",
    "    print(\"Final result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "aiplatform_project = os.getenv(\"PROJECT_ID\")\n",
    "aiplatform_location = os.getenv(\"LOCATION_ID\")\n",
    "processor_id = os.getenv(\"PROCESSOR_ID\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"preprocessing_credentials.json\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=aiplatform_project, location=aiplatform_location)\n",
    "\n",
    "# Initialize Gemini model\n",
    "def initialize_model():\n",
    "    print(\"Initializing Gemini model...\")\n",
    "    model = GoogleGenerativeAI(\n",
    "        model=\"gemini-1.0-pro\",\n",
    "        google_api_key=gemini_api_key,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"Gemini model initialized successfully.\")\n",
    "    return model\n",
    "\n",
    "# Generate embeddings using Vertex AI's text embedding model\n",
    "def generate_embeddings(text):\n",
    "    try:\n",
    "        if not text.strip():\n",
    "            print(f\"Skipped embedding: Text is empty or whitespace.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Generating embedding for text: {text[:50]}...\")\n",
    "        model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "        input_obj = TextEmbeddingInput(text=text)\n",
    "        embeddings = model.get_embeddings([input_obj])\n",
    "        print(\"Embedding generated successfully.\")\n",
    "        return embeddings[0].values\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {text[:50]}... Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Clean the Gemini model's response\n",
    "def clean_response(response_str):\n",
    "    print(\"Cleaning Gemini response...\")\n",
    "    response_str = re.sub(r'^```json\\n', '', response_str)\n",
    "    response_str = re.sub(r'```$', '', response_str).strip()\n",
    "\n",
    "    try:\n",
    "        response_json = json.loads(response_str)\n",
    "        print(\"Response cleaned and converted to JSON.\")\n",
    "        return response_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing the response as JSON, response was:\", response_str)\n",
    "        return {\"findings\": [], \"metadata\": {}}\n",
    "\n",
    "# Chunk and clean text from the raw input\n",
    "def chunk_and_clean_text(model, raw_text):\n",
    "    prompt = \"\"\"\n",
    "    You are a highly capable AI model tasked with cleaning and chunking the provided text.\n",
    "    Please return the response in JSON format with two keys:\n",
    "    - \"findings\": A list of valid claims or facts related to muscle training, nutrition, gym, biology, etc.\n",
    "    - \"metadata\": A dictionary containing the \"title\" key with the paper's title.\n",
    "    Here is the input text: {raw_text}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing text chunk: {raw_text[:100]}...\")\n",
    "    response_str = model(prompt.format(raw_text=raw_text))\n",
    "    response_json = clean_response(response_str)\n",
    "\n",
    "    findings = response_json.get(\"findings\", [])\n",
    "    metadata = response_json.get(\"metadata\", {})\n",
    "    print(f\"Processed text with findings: {len(findings)} items.\")\n",
    "    return {\"findings\": findings, \"metadata\": metadata}\n",
    "\n",
    "# Process all pages of a PDF\n",
    "def process_pages(model, pages):\n",
    "    full_response = {}\n",
    "    print(\"Starting to process PDF pages...\")\n",
    "\n",
    "    for idx, page in enumerate(pages):\n",
    "        print(f\"Processing page {idx + 1}...\")\n",
    "        response = chunk_and_clean_text(model, page)\n",
    "\n",
    "        title = response[\"metadata\"].get(\"title\", f\"Untitled Page {idx + 1}\")\n",
    "        if title not in full_response:\n",
    "            full_response[title] = []\n",
    "        full_response[title].extend(response[\"findings\"])\n",
    "    print(\"All pages processed.\")\n",
    "    return full_response\n",
    "\n",
    "# Extract text from a PDF file\n",
    "def extract_text_from_pdf(file_path):\n",
    "    print(f\"Extracting text from PDF: {file_path}...\")\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages = [page.extract_text() for page in pdf_reader.pages]\n",
    "    print(f\"Extracted {len(pages)} pages from PDF.\")\n",
    "    return pages\n",
    "\n",
    "# Process findings with embeddings\n",
    "def process_findings_with_embeddings(findings_by_title):\n",
    "    processed = {}\n",
    "    print(\"Processing findings with embeddings...\")\n",
    "\n",
    "    for title, findings in findings_by_title.items():\n",
    "        print(f\"Processing title: {title} with {len(findings)} findings...\")\n",
    "        embeddings_list = []\n",
    "\n",
    "        for finding in findings:\n",
    "            embedding = generate_embeddings(finding)\n",
    "            if embedding:\n",
    "                embeddings_list.append({\"text\": finding, \"embedding\": embedding})\n",
    "            else:\n",
    "                print(f\"Embedding failed for finding: '{finding}' under title: '{title}'\")\n",
    "\n",
    "        processed[title] = embeddings_list\n",
    "        print(f\"Processed title: {title} with {len(embeddings_list)} embeddings.\")\n",
    "    return processed\n",
    "\n",
    "# Main function to process all PDFs in a folder\n",
    "def process_folder(folder_path):\n",
    "    model = initialize_model()\n",
    "    total_data = {\"processed\": {}, \"completed_files\": []}\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        try:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if file_name in total_data[\"completed_files\"]:\n",
    "                print(f\"Skipping already completed file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing file: {file_name}...\")\n",
    "            pages = extract_text_from_pdf(file_path)\n",
    "            findings_by_title = process_pages(model, pages)\n",
    "            embeddings_data = process_findings_with_embeddings(findings_by_title)\n",
    "\n",
    "            total_data[\"processed\"].update(embeddings_data)\n",
    "            total_data[\"completed_files\"].append(file_name)\n",
    "            print(f\"File {file_name} processed successfully.\")\n",
    "\n",
    "            # Save progress after each file\n",
    "            with open(\"progress_with_embeddings.json\", \"w\") as f:\n",
    "                json.dump(total_data, f, indent=4)\n",
    "            print(\"Progress saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    return total_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"nutrition_research_papers\"  # Replace with your folder path\n",
    "    print(\"Starting pipeline...\")\n",
    "    start_time = datetime.now()\n",
    "    result = process_folder(folder_path)\n",
    "    print(f\"Pipeline completed in {datetime.now() - start_time}.\")\n",
    "    print(\"Final result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
