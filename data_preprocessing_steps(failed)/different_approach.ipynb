{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    # Initialize the Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Specify the processor name (replace with your actual processor ID)\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    # Create the raw document request\n",
    "    raw_document = documentai.RawDocument(content=pdf_content)\n",
    "\n",
    "    # Create the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # Extract and return the text\n",
    "    document = result.document\n",
    "    text = document.text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "    response = client.classify_text(document=document)\n",
    "\n",
    "    filtered_text = []\n",
    "    for category in response.categories:\n",
    "        if any(cat in category.name for cat in categories_to_keep):\n",
    "            filtered_text.append(text)\n",
    "\n",
    "    return \" \".join(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def agentic_chunking(text, embedding_model):\n",
    "    # Split text into smaller parts for processing\n",
    "    doc = Document(page_content=text)\n",
    "\n",
    "    # Use a summarization chain to group text into chunks\n",
    "    summarize_chain = load_summarize_chain(embedding_model)\n",
    "    output = summarize_chain.run([doc])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbedding\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embedding_model = TextEmbedding.from_pretrained(\"textembedding-gecko\")\n",
    "    return embedding_model.embed(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file):\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(file_path, project_id)\n",
    "\n",
    "    # Filter useful content\n",
    "    filtered_text = filter_relevant_content(text, categories_to_keep)\n",
    "\n",
    "    # Perform agentic chunking\n",
    "    chunks = agentic_chunking(filtered_text, embedding_model=\"gemini-text-embedding\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    chunk_data = [{\"chunk\": chunk, \"embedding\": generate_embeddings(chunk)} for chunk in chunks]\n",
    "\n",
    "    # Save chunks to file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_research_paper(\n",
    "    \"nutrition_research_papers/nutrients-11-01136.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from langchain.chains import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from vertexai.language_models import TextEmbedding\n",
    "\n",
    "# Set the environment variable for GCP credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'preprocessing_credentials.json'\n",
    "\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    # Initialize the Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Specify the processor name (replace with your actual processor ID)\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    # Create the raw document request\n",
    "    raw_document = documentai.RawDocument(content=pdf_content)\n",
    "\n",
    "    # Create the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # Extract and return the text\n",
    "    document = result.document\n",
    "    text = document.text\n",
    "    return text\n",
    "\n",
    "def split_document_into_chunks(file_path, max_pages=15):\n",
    "    # This function splits the PDF into smaller chunks based on max_pages\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    # Read the PDF document\n",
    "    reader = PdfReader(file_path)\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "    chunks = []\n",
    "    for start_page in range(0, num_pages, max_pages):\n",
    "        end_page = min(start_page + max_pages, num_pages)\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # Combine text from the pages in the current chunk\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page = reader.pages[page_num]\n",
    "            chunk += page.extract_text()\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def agentic_chunking(text, embedding_model):\n",
    "    # Split text into smaller parts for processing\n",
    "    doc = Document(page_content=text)\n",
    "\n",
    "    # Use a summarization chain to group text into chunks\n",
    "    summarize_chain = load_summarize_chain(embedding_model)\n",
    "    output = summarize_chain.run([doc])\n",
    "\n",
    "    return output\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    # Generate embeddings using the Gemini model\n",
    "    embedding_model = TextEmbedding.from_pretrained(\"textembedding-gecko\")\n",
    "    return embedding_model.embed(text)\n",
    "\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    # Save the chunks and embeddings to a JSON file\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n",
    "\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    # Filter the text based on the categories provided\n",
    "    # You can implement a more specific filtering method here if needed\n",
    "    filtered_text = \"\\n\".join([line for line in text.splitlines() if any(category in line for category in categories_to_keep)])\n",
    "    return filtered_text\n",
    "\n",
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file, max_pages=15):\n",
    "    # Split the document into smaller chunks if it's too large\n",
    "    chunks = split_document_into_chunks(file_path, max_pages)\n",
    "    \n",
    "    # Combine extracted text from all chunks\n",
    "    combined_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        # Filter useful content based on categories\n",
    "        filtered_text = filter_relevant_content(chunk, categories_to_keep)\n",
    "\n",
    "        # Combine the filtered text\n",
    "        combined_text += filtered_text\n",
    "\n",
    "    # Perform agentic chunking\n",
    "    chunked_text = agentic_chunking(combined_text, embedding_model=\"gemini-text-embedding\")\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    chunk_data = [{\"chunk\": chunk, \"embedding\": generate_embeddings(chunk)} for chunk in chunked_text]\n",
    "\n",
    "    # Save chunks and embeddings to a file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_research_paper(\n",
    "    \"nutrition_research_papers/nutrients-11-01136.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Endpoint `projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Endpoint `projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id` not found.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.215.95:443 {grpc_message:\"Endpoint `projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id` not found.\", grpc_status:5, created_time:\"2025-01-19T16:56:49.742892-05:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Chunks saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[43mprocess_research_paper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresistant_research_papers/2102.00836v2.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mathlyze-446917\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHealth & Fitness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNutrition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSports Science\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhysiology\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMedical Sciences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_research_chunks.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     87\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 68\u001b[0m, in \u001b[0;36mprocess_research_paper\u001b[0;34m(file_path, project_id, categories_to_keep, output_file)\u001b[0m\n\u001b[1;32m     65\u001b[0m filtered_text \u001b[38;5;241m=\u001b[39m filter_relevant_content(text, categories_to_keep)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Summarize the filtered text\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m summarized_text \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Generate embeddings for the summarized text\u001b[39;00m\n\u001b[1;32m     71\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m generate_embeddings(summarized_text)\n",
      "Cell \u001b[0;32mIn[39], line 49\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Replace with your actual model and endpoint ID for summarization\u001b[39;00m\n\u001b[1;32m     48\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mEndpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mpredictions[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:2293\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   2285\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2286\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         model_version_id\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2290\u001b[0m     )\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2293\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m   2300\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:887\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 Endpoint `projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id` not found."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "# Set up the API credentials (ensure your Google Cloud credentials are set)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "aiplatform.init(project=\"athlyze-446917\", location=\"us-central1\")  # Replace with your project ID and region\n",
    "\n",
    "# Function to extract text from PDF using Document AI\n",
    "def extract_text_from_pdf(file_path, project_id, location=\"us\", processor_id=\"a370be5d003f980f\"):\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_content = pdf_file.read()\n",
    "\n",
    "    raw_document = documentai.RawDocument(\n",
    "        content=pdf_content,\n",
    "        mime_type=\"application/pdf\"\n",
    "    )\n",
    "\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=processor_name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "    document = result.document\n",
    "    return document.text\n",
    "\n",
    "# Function to filter relevant content\n",
    "def filter_relevant_content(text, categories_to_keep):\n",
    "    \"\"\"Filter the content based on categories (customize as needed).\"\"\"\n",
    "    return text if any(cat in text for cat in categories_to_keep) else \"\"\n",
    "\n",
    "# Function to generate embeddings using Vertex AI\n",
    "def generate_embeddings(text):\n",
    "    \"\"\"Generate embeddings using Vertex AI.\"\"\"\n",
    "    # Replace with your actual model and endpoint ID for embeddings\n",
    "    endpoint = aiplatform.Endpoint(\"projects/athlyze-446917/locations/us-central1/endpoints/embedding-endpoint-id\")\n",
    "    response = endpoint.predict(instances=[{\"content\": text}])\n",
    "    return response.predictions\n",
    "\n",
    "# Function to summarize text using Vertex AI\n",
    "def summarize_text(text):\n",
    "    \"\"\"Summarize the text using Vertex AI.\"\"\"\n",
    "    # Replace with your actual model and endpoint ID for summarization\n",
    "    endpoint = aiplatform.Endpoint(\"projects/athlyze-446917/locations/us-central1/endpoints/text-bison-endpoint-id\")\n",
    "    response = endpoint.predict(instances=[{\"content\": text}])\n",
    "    return response.predictions[0]['summary']\n",
    "\n",
    "# Function to save the chunks with embeddings to a file\n",
    "def save_chunks_to_file(chunks, file_path):\n",
    "    \"\"\"Save chunked data with embeddings to a JSON file.\"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(chunks, f)\n",
    "\n",
    "# Main function to process research paper and generate vector database\n",
    "def process_research_paper(file_path, project_id, categories_to_keep, output_file):\n",
    "    \"\"\"Process a research paper, clean, chunk, summarize, and generate embeddings.\"\"\"\n",
    "    # Extract text from the paper\n",
    "    text = extract_text_from_pdf(file_path, project_id)\n",
    "\n",
    "    # Filter and clean text based on categories\n",
    "    filtered_text = filter_relevant_content(text, categories_to_keep)\n",
    "\n",
    "    # Summarize the filtered text\n",
    "    summarized_text = summarize_text(filtered_text)\n",
    "\n",
    "    # Generate embeddings for the summarized text\n",
    "    embeddings = generate_embeddings(summarized_text)\n",
    "\n",
    "    # Create chunks with embeddings\n",
    "    chunk_data = [{\"chunk\": summarized_text, \"embedding\": embeddings}]\n",
    "\n",
    "    # Save chunks with embeddings to file\n",
    "    save_chunks_to_file(chunk_data, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Chunks saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_research_paper(\n",
    "    \"resistant_research_papers/2102.00836v2.pdf\", \n",
    "    \"athlyze-446917\", \n",
    "    [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"], \n",
    "    \"test_research_chunks.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict, Union, Optional\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AthlyzeChunker:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the AthlyzeChunker with configuration.\"\"\"\n",
    "        self.chunks: Dict[str, Dict] = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.categories = [\"Health & Fitness\", \"Nutrition\", \"Sports Science\", \"Physiology\", \"Medical Sciences\"]\n",
    "        self.print_logging = True\n",
    "\n",
    "        # Initialize the Google Gemini LLM\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=\"gemini-1.0-pro\",\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_file_path: str) -> str:\n",
    "        \"\"\"Extract raw text from the PDF.\"\"\"\n",
    "        try:\n",
    "            with open(pdf_file_path, 'rb') as file:\n",
    "                reader = PdfReader(file)\n",
    "                return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_and_categorize(self, text: str) -> None:\n",
    "        \"\"\"Extract, clean, and categorize text into meaningful sections.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an assistant categorizing research into sections.\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                f\"Process the following text, clean it (e.g., remove references like [1], figures, and irrelevant details), and categorize it into: {', '.join(self.categories)}. Provide key points for each category.\"\n",
    "            ),\n",
    "            (\"user\", f\"Text: {text.replace('{', '{{').replace('}', '}}')}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format_prompt().to_messages())\n",
    "            cleaned_text = response.get('content', '').strip()\n",
    "            if self.print_logging:\n",
    "                print(\"[INFO] Text processed successfully.\")\n",
    "            self.add_chunks_by_category(cleaned_text)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Text processing failed: {e}\")\n",
    "\n",
    "    def add_chunks_by_category(self, processed_text: str) -> None:\n",
    "        \"\"\"Organize processed text into categories and subcategories.\"\"\"\n",
    "        for category in self.categories:\n",
    "            category_text = self.extract_category_text(processed_text, category)\n",
    "            if category_text:\n",
    "                chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "                self.chunks[chunk_id] = {\n",
    "                    \"category\": category,\n",
    "                    \"content\": category_text,\n",
    "                    \"metadata\": self._generate_metadata(category_text),\n",
    "                    \"chunk_index\": len(self.chunks)\n",
    "                }\n",
    "                if self.print_logging:\n",
    "                    print(f\"[INFO] Added chunk for category '{category}': {chunk_id}\")\n",
    "\n",
    "    def extract_category_text(self, text: str, category: str) -> str:\n",
    "        \"\"\"Extract text specific to a category using Gemini.\"\"\"\n",
    "        try:\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", f\"Extract key details relevant to {category}.\"),\n",
    "                (\"user\", f\"Text: {text.replace('{', '{{').replace('}', '}}')}\")\n",
    "            ])\n",
    "            response = self.llm.invoke(prompt.format_prompt().to_messages())\n",
    "            return response.get('content', '').strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract category text for {category}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _generate_metadata(self, content: str) -> Dict:\n",
    "        \"\"\"Generate metadata for a chunk.\"\"\"\n",
    "        return {\n",
    "            \"content_type\": \"scientific_finding\",\n",
    "            \"creation_time\": str(uuid.uuid1()),\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "\n",
    "    def save_chunks_to_file(self, file_path: str) -> None:\n",
    "        \"\"\"Save categorized chunks to a file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for chunk_id, chunk in self.chunks.items():\n",
    "                    file.write(f\"Chunk ID: {chunk_id}\\n\")\n",
    "                    file.write(f\"Category: {chunk['category']}\\n\")\n",
    "                    file.write(f\"Content:\\n{chunk['content']}\\n\")\n",
    "                    file.write(f\"Metadata: {chunk['metadata']}\\n\")\n",
    "                    file.write(\"\\n---\\n\\n\")\n",
    "            if self.print_logging:\n",
    "                print(f\"[INFO] Chunks saved to file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to save chunks: {e}\")\n",
    "\n",
    "# Main Function for Athlyze\n",
    "def process_research_paper_for_athlyze(pdf_path: str, output_file: str) -> None:\n",
    "    \"\"\"Extract and categorize research data into sections for Athlyze.\"\"\"\n",
    "    try:\n",
    "        chunker = AthlyzeChunker()\n",
    "        raw_text = chunker.extract_text_from_pdf(pdf_path)\n",
    "        chunker.process_and_categorize(raw_text)\n",
    "        chunker.save_chunks_to_file(output_file)\n",
    "        print(f\"[INFO] Processing completed. Results saved to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process research paper: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Replace with your PDF path\n",
    "output_file = \"athlyze_chunks.txt\"\n",
    "process_research_paper_for_athlyze(pdf_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel \n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= 'preprocessing_credentials.json'\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    text: str\n",
    "    use_case: str\n",
    "    metadata: Dict\n",
    "    section_type: str\n",
    "    relevance_score: float\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    methodology_details: Optional[Dict]\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two text chunks.\"\"\"\n",
    "        tfidf_matrix = self.tfidf.fit_transform([text1, text2])\n",
    "        return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.3) -> List[str]:\n",
    "        \"\"\"Merge chunks that are semantically similar.\"\"\"\n",
    "        merged_chunks = []\n",
    "        current_chunk = chunks[0]\n",
    "\n",
    "        for next_chunk in chunks[1:]:\n",
    "            similarity = self.get_semantic_similarity(current_chunk, next_chunk)\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk = f\"{current_chunk}\\n{next_chunk}\"\n",
    "            else:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks\n",
    "\n",
    "class AcademicPaperProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str = \"us-central1\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        self.project_id = project_id\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "\n",
    "    def extract_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract metadata from paper header/title section.\"\"\"\n",
    "        prompt = \"\"\"You are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\n",
    "        Focus on accuracy and completeness.\n",
    "\n",
    "        Required fields:\n",
    "        1. Title (exact paper title)\n",
    "        2. Authors (full list with affiliations if available)\n",
    "        3. Publication details:\n",
    "           - Year\n",
    "           - Journal/Conference\n",
    "           - DOI\n",
    "           - Volume/Issue\n",
    "        4. Keywords (if present)\n",
    "        5. Research domain/field\n",
    "\n",
    "        Format the response as a valid Python dictionary.\n",
    "\n",
    "        Text to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(text=text[:3000]),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            return eval(response.text)\n",
    "        except:\n",
    "            return {\"title\": \"Unknown\", \"authors\": [], \"year\": None}\n",
    "\n",
    "    def analyze_chunk(self, chunk: str, metadata: Dict) -> ProcessedChunk:\n",
    "        \"\"\"Analyze chunk content with enhanced agentic understanding.\"\"\"\n",
    "        prompt = \"\"\"You are an expert research analyst. Analyze this academic paper excerpt.\n",
    "\n",
    "        Task: Extract and structure the following components:\n",
    "\n",
    "        1. Core Scientific Content:\n",
    "           - Main findings or theoretical concepts\n",
    "           - Methodologies or approaches\n",
    "           - Evidence supporting claims\n",
    "\n",
    "        2. Practical Applications:\n",
    "           - How this information can be applied\n",
    "\n",
    "        3. Critical Analysis:\n",
    "           - Scientific validity (0-1)\n",
    "           - Practical applicability (0-1)\n",
    "           - Identify limitations or constraints\n",
    "\n",
    "        Format response as a Python dictionary with keys:\n",
    "        {\n",
    "            'text': 'core scientific content',\n",
    "            'use_case': 'practical applications',\n",
    "            'section_type': 'type of section',\n",
    "            'relevance_score': float,\n",
    "            'key_findings': [list of findings],\n",
    "            'citations': [list of referenced papers],\n",
    "            'methodology_details': {dict of methods}\n",
    "        }\n",
    "\n",
    "        Text to analyze:\n",
    "        {chunk}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(chunk=chunk),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = eval(response.text)\n",
    "            return ProcessedChunk(\n",
    "                text=result['text'],\n",
    "                use_case=result['use_case'],\n",
    "                metadata=metadata,\n",
    "                section_type=result['section_type'],\n",
    "                relevance_score=result['relevance_score'],\n",
    "                key_findings=result['key_findings'],\n",
    "                citations=result['citations'],\n",
    "                methodology_details=result['methodology_details']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            return ProcessedChunk(\n",
    "                text=chunk,\n",
    "                use_case=\"\",\n",
    "                metadata=metadata,\n",
    "                section_type=\"unknown\",\n",
    "                relevance_score=0.0,\n",
    "                key_findings=[],\n",
    "                citations=[],\n",
    "                methodology_details=None\n",
    "            )\n",
    "\n",
    "    def agentic_chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Perform intelligent chunking based on semantic meaning.\"\"\"\n",
    "        section_pattern = r'\\n#{1,3}\\s+[A-Z].*?\\n'\n",
    "        sections = re.split(section_pattern, text)\n",
    "\n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            initial_chunks = self.text_splitter.create_documents([section])\n",
    "            chunk_texts = [chunk.page_content for chunk in initial_chunks]\n",
    "            merged_chunks = self.semantic_chunker.merge_similar_chunks(chunk_texts)\n",
    "            chunks.extend(merged_chunks)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[ProcessedChunk]:\n",
    "        \"\"\"Process a PDF file with enhanced chunking.\"\"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            doc = PyPDF2.PdfReader(file)  # Pass the file object, not just the path\n",
    "            text = \"\"\n",
    "            for page in doc.pages:  # Iterate over the pages\n",
    "                text += page.extract_text()  # Use extract_text()\n",
    "\n",
    "        metadata = self.extract_metadata(text)\n",
    "        chunks = self.agentic_chunk_text(text)\n",
    "\n",
    "        processed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            processed_chunk = self.analyze_chunk(chunk, metadata)\n",
    "            if processed_chunk.relevance_score > 0.6:\n",
    "                processed_chunks.append(processed_chunk)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "    def vectorize_chunks(self, chunks: List[ProcessedChunk]) -> List[Dict]:\n",
    "        \"\"\"Enhanced vectorization with semantic understanding.\"\"\"\n",
    "        vectorized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            combined_text = f\"\"\"\n",
    "            Content: {chunk.text}\n",
    "\n",
    "            Key Findings: {' | '.join(chunk.key_findings)}\n",
    "\n",
    "            Practical Applications: {chunk.use_case}\n",
    "\n",
    "            Methodology: {chunk.methodology_details if chunk.methodology_details else 'Not specified'}\n",
    "\n",
    "            Section Type: {chunk.section_type}\n",
    "            \"\"\"\n",
    "\n",
    "            metadata = {\n",
    "                **chunk.metadata,\n",
    "                \"section_type\": chunk.section_type,\n",
    "                \"relevance_score\": chunk.relevance_score,\n",
    "                \"use_case\": chunk.use_case,\n",
    "                \"key_findings\": chunk.key_findings,\n",
    "                \"citations\": chunk.citations,\n",
    "                \"methodology_details\": chunk.methodology_details\n",
    "            }\n",
    "\n",
    "            vectorized_chunks.append({\n",
    "                \"text\": combined_text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "        return vectorized_chunks\n",
    "\n",
    "class ChunkInspector:\n",
    "    def __init__(self, processor: AcademicPaperProcessor):\n",
    "        self.processor = processor\n",
    "        self.console = Console()\n",
    "\n",
    "    def inspect_chunks(self, pdf_path: str, max_chunks: int = 5):\n",
    "        \"\"\"Inspect chunks and embeddings before upload.\"\"\"\n",
    "        processed_chunks = self.processor.process_pdf(pdf_path)\n",
    "        vectors = self.processor.vectorize_chunks(processed_chunks)\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Chunk Processing Summary:[/bold blue]\")\n",
    "        self.console.print(f\"Total chunks extracted: {len(vectors)}\")\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Sample Chunks:[/bold blue]\")\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Chunk #\", style=\"dim\")\n",
    "        table.add_column(\"Text Preview\")\n",
    "        table.add_column(\"Relevance Score\")\n",
    "        table.add_column(\"Section Type\")\n",
    "        table.add_column(\"Key Findings Count\")\n",
    "\n",
    "        for i, vector in enumerate(vectors[:max_chunks]):\n",
    "            metadata = vector['metadata']\n",
    "            table.add_row(\n",
    "                str(i + 1),\n",
    "                vector['text'][:100] + \"...\",\n",
    "                f\"{metadata['relevance_score']:.2f}\",\n",
    "                metadata['section_type'],\n",
    "                str(len(metadata.get('key_findings', [])))\n",
    "            )\n",
    "\n",
    "        self.console.print(table)\n",
    "\n",
    "    def export_to_csv(self, vectors: List[Dict], output_path: str):\n",
    "        \"\"\"Export chunks and metadata to CSV.\"\"\"\n",
    "        records = []\n",
    "        for vector in vectors:\n",
    "            metadata = vector['metadata']\n",
    "            records.append({\n",
    "                'text_preview': vector['text'][:200],\n",
    "                'section_type': metadata['section_type'],\n",
    "                'relevance_score': metadata['relevance_score'],\n",
    "                'key_findings_count': len(metadata.get('key_findings', []))\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        self.console.print(f\"\\n[green]Exported analysis to {output_path}[/green]\")\n",
    "\n",
    "def main():\n",
    "    processor = AcademicPaperProcessor(\n",
    "        project_id=\"athlyze-446917\",\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    inspector = ChunkInspector(processor)\n",
    "\n",
    "    pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "    inspector.inspect_chunks(pdf_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown model publishers/google/models/gemini-pro; {'gs://google-cloud-aiplatform/schema/predict/instance/text_generation_1.0.0.yaml': <class 'vertexai.language_models.TextGenerationModel'>}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m     inspector\u001b[38;5;241m.\u001b[39minspect_chunks(pdf_path)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 297\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 297\u001b[0m     processor \u001b[38;5;241m=\u001b[39m \u001b[43mAcademicPaperProcessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mathlyze-446917\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-central1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     inspector \u001b[38;5;241m=\u001b[39m ChunkInspector(processor)\n\u001b[1;32m    303\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresistant_research_papers/2102.00836v2.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[42], line 74\u001b[0m, in \u001b[0;36mAcademicPaperProcessor.__init__\u001b[0;34m(self, project_id, location, chunk_size, chunk_overlap)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_id \u001b[38;5;241m=\u001b[39m project_id\n\u001b[1;32m     73\u001b[0m vertexai\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mproject_id, location\u001b[38;5;241m=\u001b[39mlocation)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTextGenerationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-pro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Updated to Gemini model\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m     76\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m     77\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39mchunk_overlap,\n\u001b[1;32m     78\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m## \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_chunker \u001b[38;5;241m=\u001b[39m SemanticChunker()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:289\u001b[0m, in \u001b[0;36m_ModelGardenModel.from_pretrained\u001b[0;34m(cls, model_name)\u001b[0m\n\u001b[1;32m    278\u001b[0m credential_exception_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUnable to authenticate your request.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDepending on your runtime environment, you can complete authentication by:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError(credential_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:206\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[0;34m(interface_class, model_name, publisher_model, tuned_vertex_model)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interface_class\u001b[38;5;241m.\u001b[39m_INSTANCE_SCHEMA_URI:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterface_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a correct model interface class since it does not have an instance schema URI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         )\n\u001b[0;32m--> 206\u001b[0m     model_info \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_to_class_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_INSTANCE_SCHEMA_URI\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     schema_uri \u001b[38;5;241m=\u001b[39m publisher_model\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mpredict_schemata\u001b[38;5;241m.\u001b[39minstance_schema_uri\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athlyze/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:168\u001b[0m, in \u001b[0;36m_get_model_info\u001b[0;34m(model_id, schema_to_class_map, interface_class, publisher_model_res, tuned_vertex_model)\u001b[0m\n\u001b[1;32m    163\u001b[0m     interface_class \u001b[38;5;241m=\u001b[39m schema_to_class_map\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    164\u001b[0m         publisher_model_res\u001b[38;5;241m.\u001b[39mpredict_schemata\u001b[38;5;241m.\u001b[39minstance_schema_uri\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interface_class:\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublisher_model_res\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_to_class_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         )\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ModelInfo(\n\u001b[1;32m    173\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mendpoint_name,\n\u001b[1;32m    174\u001b[0m     interface_class\u001b[38;5;241m=\u001b[39minterface_class,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     tuning_model_id\u001b[38;5;241m=\u001b[39mtuning_model_id,\n\u001b[1;32m    178\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown model publishers/google/models/gemini-pro; {'gs://google-cloud-aiplatform/schema/predict/instance/text_generation_1.0.0.yaml': <class 'vertexai.language_models.TextGenerationModel'>}"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'preprocessing_credentials.json'\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    text: str\n",
    "    use_case: str\n",
    "    metadata: Dict\n",
    "    section_type: str\n",
    "    relevance_score: float\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    methodology_details: Optional[Dict]\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two text chunks.\"\"\"\n",
    "        tfidf_matrix = self.tfidf.fit_transform([text1, text2])\n",
    "        return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.3) -> List[str]:\n",
    "        \"\"\"Merge chunks that are semantically similar.\"\"\"\n",
    "        merged_chunks = []\n",
    "        current_chunk = chunks[0]\n",
    "\n",
    "        for next_chunk in chunks[1:]:\n",
    "            similarity = self.get_semantic_similarity(current_chunk, next_chunk)\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk = f\"{current_chunk}\\n{next_chunk}\"\n",
    "            else:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks\n",
    "\n",
    "class AcademicPaperProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str = \"us-central1\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        self.project_id = project_id\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = TextGenerationModel.from_pretrained(\"gemini-pro\")  # Updated to Gemini model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "\n",
    "    def extract_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract metadata from paper header/title section.\"\"\"\n",
    "        prompt = \"\"\"You are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\n",
    "        Focus on accuracy and completeness.\n",
    "\n",
    "        Required fields:\n",
    "        1. Title (exact paper title)\n",
    "        2. Authors (full list with affiliations if available)\n",
    "        3. Publication details:\n",
    "           - Year\n",
    "           - Journal/Conference\n",
    "           - DOI\n",
    "           - Volume/Issue\n",
    "        4. Keywords (if present)\n",
    "        5. Research domain/field\n",
    "\n",
    "        Format the response as a valid Python dictionary.\n",
    "\n",
    "        Text to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(text=text[:3000]),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            return eval(response.text)\n",
    "        except:\n",
    "            return {\"title\": \"Unknown\", \"authors\": [], \"year\": None}\n",
    "\n",
    "    def analyze_chunk(self, chunk: str, metadata: Dict) -> ProcessedChunk:\n",
    "        \"\"\"Analyze chunk content with enhanced agentic understanding.\"\"\"\n",
    "        prompt = \"\"\"You are an expert research analyst. Analyze this academic paper excerpt.\n",
    "\n",
    "        Task: Extract and structure the following components:\n",
    "\n",
    "        1. Core Scientific Content:\n",
    "           - Main findings or theoretical concepts\n",
    "           - Methodologies or approaches\n",
    "           - Evidence supporting claims\n",
    "\n",
    "        2. Practical Applications:\n",
    "           - How this information can be applied\n",
    "\n",
    "        3. Critical Analysis:\n",
    "           - Scientific validity (0-1)\n",
    "           - Practical applicability (0-1)\n",
    "           - Identify limitations or constraints\n",
    "\n",
    "        Format response as a Python dictionary with keys:\n",
    "        {\n",
    "            'text': 'core scientific content',\n",
    "            'use_case': 'practical applications',\n",
    "            'section_type': 'type of section',\n",
    "            'relevance_score': float,\n",
    "            'key_findings': [list of findings],\n",
    "            'citations': [list of referenced papers],\n",
    "            'methodology_details': {dict of methods}\n",
    "        }\n",
    "\n",
    "        Text to analyze:\n",
    "        {chunk}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(chunk=chunk),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = eval(response.text)\n",
    "            return ProcessedChunk(\n",
    "                text=result['text'],\n",
    "                use_case=result['use_case'],\n",
    "                metadata=metadata,\n",
    "                section_type=result['section_type'],\n",
    "                relevance_score=result['relevance_score'],\n",
    "                key_findings=result['key_findings'],\n",
    "                citations=result['citations'],\n",
    "                methodology_details=result['methodology_details']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            return ProcessedChunk(\n",
    "                text=chunk,\n",
    "                use_case=\"\",\n",
    "                metadata=metadata,\n",
    "                section_type=\"unknown\",\n",
    "                relevance_score=0.0,\n",
    "                key_findings=[],\n",
    "                citations=[],\n",
    "                methodology_details=None\n",
    "            )\n",
    "\n",
    "    def agentic_chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Perform intelligent chunking based on semantic meaning.\"\"\"\n",
    "        section_pattern = r'\\n#{1,3}\\s+[A-Z].*?\\n'\n",
    "        sections = re.split(section_pattern, text)\n",
    "\n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            initial_chunks = self.text_splitter.create_documents([section])\n",
    "            chunk_texts = [chunk.page_content for chunk in initial_chunks]\n",
    "            merged_chunks = self.semantic_chunker.merge_similar_chunks(chunk_texts)\n",
    "            chunks.extend(merged_chunks)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[ProcessedChunk]:\n",
    "        \"\"\"Process a PDF file with enhanced chunking.\"\"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            doc = PyPDF2.PdfReader(file)  # Pass the file object, not just the path\n",
    "            text = \"\"\n",
    "            for page in doc.pages:  # Iterate over the pages\n",
    "                text += page.extract_text()  # Use extract_text()\n",
    "\n",
    "        metadata = self.extract_metadata(text)\n",
    "        chunks = self.agentic_chunk_text(text)\n",
    "\n",
    "        processed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            processed_chunk = self.analyze_chunk(chunk, metadata)\n",
    "            if processed_chunk.relevance_score > 0.6:\n",
    "                processed_chunks.append(processed_chunk)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "    def vectorize_chunks(self, chunks: List[ProcessedChunk]) -> List[Dict]:\n",
    "        \"\"\"Enhanced vectorization with semantic understanding.\"\"\"\n",
    "        vectorized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            combined_text = f\"\"\"\n",
    "            Content: {chunk.text}\n",
    "\n",
    "            Key Findings: {' | '.join(chunk.key_findings)}\n",
    "\n",
    "            Practical Applications: {chunk.use_case}\n",
    "\n",
    "            Methodology: {chunk.methodology_details if chunk.methodology_details else 'Not specified'}\n",
    "\n",
    "            Section Type: {chunk.section_type}\n",
    "            \"\"\"\n",
    "\n",
    "            metadata = {\n",
    "                **chunk.metadata,\n",
    "                \"section_type\": chunk.section_type,\n",
    "                \"relevance_score\": chunk.relevance_score,\n",
    "                \"use_case\": chunk.use_case,\n",
    "                \"key_findings\": chunk.key_findings,\n",
    "                \"citations\": chunk.citations,\n",
    "                \"methodology_details\": chunk.methodology_details\n",
    "            }\n",
    "\n",
    "            vectorized_chunks.append({\n",
    "                \"text\": combined_text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "        return vectorized_chunks\n",
    "\n",
    "class ChunkInspector:\n",
    "    def __init__(self, processor: AcademicPaperProcessor):\n",
    "        self.processor = processor\n",
    "        self.console = Console()\n",
    "\n",
    "    def inspect_chunks(self, pdf_path: str, max_chunks: int = 5):\n",
    "        \"\"\"Inspect chunks and embeddings before upload.\"\"\"\n",
    "        processed_chunks = self.processor.process_pdf(pdf_path)\n",
    "        vectors = self.processor.vectorize_chunks(processed_chunks)\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Chunk Processing Summary:[/bold blue]\")\n",
    "        self.console.print(f\"Total chunks extracted: {len(vectors)}\")\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Sample Chunks:[/bold blue]\")\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Chunk #\", style=\"dim\")\n",
    "        table.add_column(\"Text Preview\")\n",
    "        table.add_column(\"Relevance Score\")\n",
    "        table.add_column(\"Section Type\")\n",
    "        table.add_column(\"Key Findings Count\")\n",
    "\n",
    "        for i, vector in enumerate(vectors[:max_chunks]):\n",
    "            metadata = vector['metadata']\n",
    "            table.add_row(\n",
    "                str(i + 1),\n",
    "                vector['text'][:100] + \"...\",\n",
    "                f\"{metadata['relevance_score']:.2f}\",\n",
    "                metadata['section_type'],\n",
    "                str(len(metadata.get('key_findings', [])))\n",
    "            )\n",
    "\n",
    "        self.console.print(table)\n",
    "\n",
    "    def export_to_csv(self, vectors: List[Dict], output_path: str):\n",
    "        \"\"\"Export chunks and metadata to CSV.\"\"\"\n",
    "        records = []\n",
    "        for vector in vectors:\n",
    "            metadata = vector['metadata']\n",
    "            records.append({\n",
    "                'text_preview': vector['text'][:200],\n",
    "                'section_type': metadata['section_type'],\n",
    "                'relevance_score': metadata['relevance_score'],\n",
    "                'key_findings_count': len(metadata.get('key_findings', []))\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        self.console.print(f\"\\n[green]Exported analysis to {output_path}[/green]\")\n",
    "\n",
    "def main():\n",
    "    processor = AcademicPaperProcessor(\n",
    "        project_id=\"athlyze-446917\",\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    inspector = ChunkInspector(processor)\n",
    "\n",
    "    pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "    inspector.inspect_chunks(pdf_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'preprocessing_credentials.json'\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    text: str\n",
    "    use_case: str\n",
    "    metadata: Dict\n",
    "    section_type: str\n",
    "    relevance_score: float\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    methodology_details: Optional[Dict]\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two text chunks.\"\"\"\n",
    "        tfidf_matrix = self.tfidf.fit_transform([text1, text2])\n",
    "        return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.3) -> List[str]:\n",
    "        \"\"\"Merge chunks that are semantically similar.\"\"\"\n",
    "        merged_chunks = []\n",
    "        current_chunk = chunks[0]\n",
    "\n",
    "        for next_chunk in chunks[1:]:\n",
    "            similarity = self.get_semantic_similarity(current_chunk, next_chunk)\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk = f\"{current_chunk}\\n{next_chunk}\"\n",
    "            else:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks\n",
    "\n",
    "class AcademicPaperProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str = \"us-central1\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        self.project_id = project_id\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = TextGenerationModel.from_pretrained(\"gemini-pro\")  # Updated to Gemini model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "\n",
    "    def extract_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract metadata from paper header/title section.\"\"\"\n",
    "        prompt = \"\"\"You are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\n",
    "        Focus on accuracy and completeness.\n",
    "\n",
    "        Required fields:\n",
    "        1. Title (exact paper title)\n",
    "        2. Authors (full list with affiliations if available)\n",
    "        3. Publication details:\n",
    "           - Year\n",
    "           - Journal/Conference\n",
    "           - DOI\n",
    "           - Volume/Issue\n",
    "        4. Keywords (if present)\n",
    "        5. Research domain/field\n",
    "\n",
    "        Format the response as a valid Python dictionary.\n",
    "\n",
    "        Text to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(text=text[:3000]),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            return eval(response.text)\n",
    "        except:\n",
    "            return {\"title\": \"Unknown\", \"authors\": [], \"year\": None}\n",
    "\n",
    "    def analyze_chunk(self, chunk: str, metadata: Dict) -> ProcessedChunk:\n",
    "        \"\"\"Analyze chunk content with enhanced agentic understanding.\"\"\"\n",
    "        prompt = \"\"\"You are an expert research analyst. Analyze this academic paper excerpt.\n",
    "\n",
    "        Task: Extract and structure the following components:\n",
    "\n",
    "        1. Core Scientific Content:\n",
    "           - Main findings or theoretical concepts\n",
    "           - Methodologies or approaches\n",
    "           - Evidence supporting claims\n",
    "\n",
    "        2. Practical Applications:\n",
    "           - How this information can be applied\n",
    "\n",
    "        3. Critical Analysis:\n",
    "           - Scientific validity (0-1)\n",
    "           - Practical applicability (0-1)\n",
    "           - Identify limitations or constraints\n",
    "\n",
    "        Format response as a Python dictionary with keys:\n",
    "        {\n",
    "            'text': 'core scientific content',\n",
    "            'use_case': 'practical applications',\n",
    "            'section_type': 'type of section',\n",
    "            'relevance_score': float,\n",
    "            'key_findings': [list of findings],\n",
    "            'citations': [list of referenced papers],\n",
    "            'methodology_details': {dict of methods}\n",
    "        }\n",
    "\n",
    "        Text to analyze:\n",
    "        {chunk}\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.model.predict(\n",
    "            prompt.format(chunk=chunk),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = eval(response.text)\n",
    "            return ProcessedChunk(\n",
    "                text=result['text'],\n",
    "                use_case=result['use_case'],\n",
    "                metadata=metadata,\n",
    "                section_type=result['section_type'],\n",
    "                relevance_score=result['relevance_score'],\n",
    "                key_findings=result['key_findings'],\n",
    "                citations=result['citations'],\n",
    "                methodology_details=result['methodology_details']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            return ProcessedChunk(\n",
    "                text=chunk,\n",
    "                use_case=\"\",\n",
    "                metadata=metadata,\n",
    "                section_type=\"unknown\",\n",
    "                relevance_score=0.0,\n",
    "                key_findings=[],\n",
    "                citations=[],\n",
    "                methodology_details=None\n",
    "            )\n",
    "\n",
    "    def agentic_chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Perform intelligent chunking based on semantic meaning.\"\"\"\n",
    "        section_pattern = r'\\n#{1,3}\\s+[A-Z].*?\\n'\n",
    "        sections = re.split(section_pattern, text)\n",
    "\n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            initial_chunks = self.text_splitter.create_documents([section])\n",
    "            chunk_texts = [chunk.page_content for chunk in initial_chunks]\n",
    "            merged_chunks = self.semantic_chunker.merge_similar_chunks(chunk_texts)\n",
    "            chunks.extend(merged_chunks)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[ProcessedChunk]:\n",
    "        \"\"\"Process a PDF file with enhanced chunking.\"\"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            doc = PyPDF2.PdfReader(file)  # Pass the file object, not just the path\n",
    "            text = \"\"\n",
    "            for page in doc.pages:  # Iterate over the pages\n",
    "                text += page.extract_text()  # Use extract_text()\n",
    "\n",
    "        metadata = self.extract_metadata(text)\n",
    "        chunks = self.agentic_chunk_text(text)\n",
    "\n",
    "        processed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            processed_chunk = self.analyze_chunk(chunk, metadata)\n",
    "            if processed_chunk.relevance_score > 0.6:\n",
    "                processed_chunks.append(processed_chunk)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "    def vectorize_chunks(self, chunks: List[ProcessedChunk]) -> List[Dict]:\n",
    "        \"\"\"Enhanced vectorization with semantic understanding.\"\"\"\n",
    "        vectorized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            combined_text = f\"\"\"\n",
    "            Content: {chunk.text}\n",
    "\n",
    "            Key Findings: {' | '.join(chunk.key_findings)}\n",
    "\n",
    "            Practical Applications: {chunk.use_case}\n",
    "\n",
    "            Methodology: {chunk.methodology_details if chunk.methodology_details else 'Not specified'}\n",
    "\n",
    "            Section Type: {chunk.section_type}\n",
    "            \"\"\"\n",
    "\n",
    "            metadata = {\n",
    "                **chunk.metadata,\n",
    "                \"section_type\": chunk.section_type,\n",
    "                \"relevance_score\": chunk.relevance_score,\n",
    "                \"use_case\": chunk.use_case,\n",
    "                \"key_findings\": chunk.key_findings,\n",
    "                \"citations\": chunk.citations,\n",
    "                \"methodology_details\": chunk.methodology_details\n",
    "            }\n",
    "\n",
    "            vectorized_chunks.append({\n",
    "                \"text\": combined_text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "        return vectorized_chunks\n",
    "\n",
    "class ChunkInspector:\n",
    "    def __init__(self, processor: AcademicPaperProcessor):\n",
    "        self.processor = processor\n",
    "        self.console = Console()\n",
    "\n",
    "    def inspect_chunks(self, pdf_path: str, max_chunks: int = 5):\n",
    "        \"\"\"Inspect chunks and embeddings before upload.\"\"\"\n",
    "        processed_chunks = self.processor.process_pdf(pdf_path)\n",
    "        vectors = self.processor.vectorize_chunks(processed_chunks)\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Chunk Processing Summary:[/bold blue]\")\n",
    "        self.console.print(f\"Total chunks extracted: {len(vectors)}\")\n",
    "\n",
    "        self.console.print(\"\\n[bold blue]Sample Chunks:[/bold blue]\")\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Chunk #\", style=\"dim\")\n",
    "        table.add_column(\"Text Preview\")\n",
    "        table.add_column(\"Relevance Score\")\n",
    "        table.add_column(\"Section Type\")\n",
    "        table.add_column(\"Key Findings Count\")\n",
    "\n",
    "        for i, vector in enumerate(vectors[:max_chunks]):\n",
    "            metadata = vector['metadata']\n",
    "            table.add_row(\n",
    "                str(i + 1),\n",
    "                vector['text'][:100] + \"...\",\n",
    "                f\"{metadata['relevance_score']:.2f}\",\n",
    "                metadata['section_type'],\n",
    "                str(len(metadata.get('key_findings', [])))\n",
    "            )\n",
    "\n",
    "        self.console.print(table)\n",
    "\n",
    "    def export_to_csv(self, vectors: List[Dict], output_path: str):\n",
    "        \"\"\"Export chunks and metadata to CSV.\"\"\"\n",
    "        records = []\n",
    "        for vector in vectors:\n",
    "            metadata = vector['metadata']\n",
    "            records.append({\n",
    "                'text_preview': vector['text'][:200],\n",
    "                'section_type': metadata['section_type'],\n",
    "                'relevance_score': metadata['relevance_score'],\n",
    "                'key_findings_count': len(metadata.get('key_findings', []))\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        self.console.print(f\"\\n[green]Exported analysis to {output_path}[/green]\")\n",
    "\n",
    "def main():\n",
    "    processor = AcademicPaperProcessor(\n",
    "        project_id=\"athlyze-446917\",\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    inspector = ChunkInspector(processor)\n",
    "\n",
    "    pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"\n",
    "    inspector.inspect_chunks(pdf_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/chat-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 Chat (Legacy)',\n",
       "       description='A legacy text-only model optimized for chat conversations',\n",
       "       input_token_limit=4096,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "       temperature=0.25,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Latest',\n",
       "       description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
       "                    'February 15th, 2025. Move to a newer Gemini version.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
       "       description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
       "                    'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
       "                    'version.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
       "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
       "                    'Move to a newer Gemini version.'),\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
       "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
       "                    'Move to a newer Gemini version.'),\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
       "                    'million tokens.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro 001',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Pro 002',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-exp-0801',\n",
       "       base_model_id='',\n",
       "       version='exp-0801',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-exp-0827',\n",
       "       base_model_id='',\n",
       "       version='exp-1206',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
       "                    'across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001',\n",
       "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in May of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-001-tuning',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
       "       description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
       "                    'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
       "       input_token_limit=16384,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
       "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-exp-0827',\n",
       "       base_model_id='',\n",
       "       version='exp-1206',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Flash 002',\n",
       "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B 001',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
       "                    'released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
       "       description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
       "                    'smallest and most cost effective Flash model. Replaced by '\n",
       "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
       "       description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
       "                    'smallest and most cost effective Flash model. Replaced by '\n",
       "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='exp_1206',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-exp-1121',\n",
       "       base_model_id='',\n",
       "       version='exp-1206',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-exp-1114',\n",
       "       base_model_id='',\n",
       "       version='exp-1206',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
       "       description='Gemini 2.0 Flash Thinking Experimental',\n",
       "       input_token_limit=32767,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
       "       description='Gemini 2.0 Flash Thinking Experimental',\n",
       "       input_token_limit=32767,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/learnlm-1.5-pro-experimental',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='LearnLM 1.5 Pro Experimental',\n",
       "       description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
       "                    'mid-size multimodal model that supports up to 2 million tokens.'),\n",
       "       input_token_limit=32767,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "     \n",
    "\n",
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting metadata: invalid syntax (<string>, line 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'generate_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 223\u001b[0m\n\u001b[1;32m    221\u001b[0m processor \u001b[38;5;241m=\u001b[39m AcademicPaperProcessor()\n\u001b[1;32m    222\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresistant_research_papers/2102.00836v2.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your PDF file path\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk)\n",
      "Cell \u001b[0;32mIn[56], line 213\u001b[0m, in \u001b[0;36mAcademicPaperProcessor.process_pdf\u001b[0;34m(self, pdf_path)\u001b[0m\n\u001b[1;32m    208\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_metadata(text)\n\u001b[1;32m    209\u001b[0m sections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_chunker\u001b[38;5;241m.\u001b[39mmerge_similar_chunks(\n\u001b[1;32m    210\u001b[0m     [chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([text])]\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyze_chunk(chunk, metadata)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m sections\n\u001b[1;32m    216\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[56], line 214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_metadata(text)\n\u001b[1;32m    209\u001b[0m sections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_chunker\u001b[38;5;241m.\u001b[39mmerge_similar_chunks(\n\u001b[1;32m    210\u001b[0m     [chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([text])]\n\u001b[1;32m    211\u001b[0m )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m sections\n\u001b[1;32m    216\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[56], line 153\u001b[0m, in \u001b[0;36mAcademicPaperProcessor.analyze_chunk\u001b[0;34m(self, chunk, metadata)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Analyze chunk content using the Gemini API.\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert research analyst specializing in exercise science and nutrition. Analyze this academic paper excerpt with extreme detail and precision.\u001b[39m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124mTask: Extract and structure the following components while maintaining academic rigor:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{chunk}\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 153\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m(\n\u001b[1;32m    154\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-bison-001\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with the desired model\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(chunk\u001b[38;5;241m=\u001b[39mchunk),\n\u001b[1;32m    156\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m    157\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    160\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    161\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m    162\u001b[0m     prompt\u001b[38;5;241m.\u001b[39mformat(chunk\u001b[38;5;241m=\u001b[39mchunk),\n\u001b[1;32m    163\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgenai\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mGenerationConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'generate_text'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    text: str\n",
    "    use_case: str\n",
    "    metadata: Dict\n",
    "    section_type: str\n",
    "    relevance_score: float\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    methodology_details: Optional[Dict]\n",
    "\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\", max_features=1000, ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two text chunks.\"\"\"\n",
    "        tfidf_matrix = self.tfidf.fit_transform([text1, text2])\n",
    "        return (tfidf_matrix * tfidf_matrix.T).toarray()[0, 1]\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.3) -> List[str]:\n",
    "        \"\"\"Merge chunks that are semantically similar.\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "            \n",
    "        merged_chunks = []\n",
    "        current_chunk = chunks[0]\n",
    "\n",
    "        for next_chunk in chunks[1:]:\n",
    "            similarity = self.get_semantic_similarity(current_chunk, next_chunk)\n",
    "\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk = f\"{current_chunk}\\n{next_chunk}\"\n",
    "            else:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks\n",
    "\n",
    "\n",
    "class AcademicPaperProcessor:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        )\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "\n",
    "    def extract_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract metadata using the Gemini API.\"\"\"\n",
    "        prompt = \"\"\"You are an expert academic research parser. Extract comprehensive metadata from this academic paper text.\n",
    "        Focus on accuracy and completeness.\n",
    "        \n",
    "        Required fields:\n",
    "        1. Title (exact paper title)\n",
    "        2. Authors (full list with affiliations if available)\n",
    "        3. Publication details:\n",
    "        - Year\n",
    "        - Journal/Conference\n",
    "        - DOI\n",
    "        - Volume/Issue\n",
    "        4. Keywords (if present)\n",
    "        5. Research domain/field\n",
    "        \n",
    "        Format the response as a valid Python dictionary.\n",
    "        \n",
    "        Text to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(\n",
    "            prompt.format(text=text[:3000]),\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=1024,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            return eval(response.text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting metadata: {e}\")\n",
    "            return {\"title\": \"Unknown\", \"authors\": [], \"year\": None}\n",
    "\n",
    "    def analyze_chunk(self, chunk: str, metadata: Dict) -> ProcessedChunk:\n",
    "        \"\"\"Analyze chunk content using the Gemini API.\"\"\"\n",
    "        prompt = \"\"\"You are an expert research analyst specializing in exercise science and nutrition. Analyze this academic paper excerpt with extreme detail and precision.\n",
    "\n",
    "        Task: Extract and structure the following components while maintaining academic rigor:\n",
    "\n",
    "        1. Core Scientific Content:\n",
    "           - What are the main findings or theoretical concepts?\n",
    "           - What methodologies or approaches are described?\n",
    "           - What evidence supports the claims?\n",
    "\n",
    "        2. Practical Applications:\n",
    "           - How can this information be directly applied to fitness and nutrition tracking?\n",
    "           - What specific metrics or parameters should be monitored?\n",
    "           - How could this be implemented in a digital health platform?\n",
    "\n",
    "        3. Critical Analysis:\n",
    "           - Rate the scientific validity (0-1)\n",
    "           - Rate the practical applicability (0-1)\n",
    "           - Identify any limitations or constraints\n",
    "\n",
    "        4. Key Findings:\n",
    "           - List the most important discoveries or insights\n",
    "           - Note any numerical values or ranges that could be tracked\n",
    "           - Highlight any cause-effect relationships\n",
    "\n",
    "        5. Implementation Context:\n",
    "           - Required measurements or tracking methods\n",
    "           - Frequency of monitoring\n",
    "           - User expertise level needed\n",
    "\n",
    "        Format response as a Python dictionary with keys:\n",
    "        {\n",
    "            'text': 'core scientific content',\n",
    "            'use_case': 'practical applications',\n",
    "            'section_type': 'type of section',\n",
    "            'relevance_score': float,\n",
    "            'key_findings': [list of findings],\n",
    "            'methodology_details': {dict of methods},\n",
    "            'citations': [list of referenced papers]\n",
    "        }\n",
    "\n",
    "        Text to analyze:\n",
    "        {chunk}\n",
    "        \"\"\"\n",
    "\n",
    "        response = genai.generate_text(\n",
    "            model=\"text-bison-001\",  # Replace with the desired model\n",
    "            prompt=prompt.format(chunk=chunk),\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(\n",
    "            prompt.format(chunk=chunk),\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=2048,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = eval(response.text)\n",
    "            return ProcessedChunk(\n",
    "                text=result[\"text\"],\n",
    "                use_case=result[\"use_case\"],\n",
    "                metadata=metadata,\n",
    "                section_type=result[\"section_type\"],\n",
    "                relevance_score=result[\"relevance_score\"],\n",
    "                key_findings=result[\"key_findings\"],\n",
    "                citations=result[\"citations\"],\n",
    "                methodology_details=result[\"methodology_details\"],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            return ProcessedChunk(\n",
    "                text=chunk,\n",
    "                use_case=\"\",\n",
    "                metadata=metadata,\n",
    "                section_type=\"unknown\",\n",
    "                relevance_score=0.0,\n",
    "                key_findings=[],\n",
    "                citations=[],\n",
    "                methodology_details=None,\n",
    "            )\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[ProcessedChunk]:\n",
    "        \"\"\"Process a PDF file.\"\"\"\n",
    "        # Initialize text variable\n",
    "        text = \"\"\n",
    "        \n",
    "        # Keep file open until we're done extracting text\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            # Create PDF reader\n",
    "            doc = PyPDF2.PdfReader(file)\n",
    "            # Extract text from all pages\n",
    "            for page in doc.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "        # Now that we have the text, we can proceed with the rest of the processing\n",
    "        metadata = self.extract_metadata(text)\n",
    "        sections = self.semantic_chunker.merge_similar_chunks(\n",
    "            [chunk.page_content for chunk in self.text_splitter.create_documents([text])]\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            self.analyze_chunk(chunk, metadata)\n",
    "            for chunk in sections\n",
    "        ]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    processor = AcademicPaperProcessor()\n",
    "    pdf_path = \"resistant_research_papers/2102.00836v2.pdf\"  # Replace with your PDF file path\n",
    "    chunks = processor.process_pdf(pdf_path)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
